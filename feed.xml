<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="/dlie_knowledge_graph/feed.xml" rel="self" type="application/atom+xml" /><link href="/dlie_knowledge_graph/" rel="alternate" type="text/html" /><updated>2025-07-08T19:48:49+00:00</updated><id>/dlie_knowledge_graph/feed.xml</id><title type="html">Digital Library of Integral Ecology</title><subtitle>A tutorial series on building a knowledge graph for ecology</subtitle><entry><title type="html">Contribute: Join the Digital Library of Integral Ecology</title><link href="/dlie_knowledge_graph/2025/07/04/How-you-can-help-build-and-grow-this-open-knowledge-commons" rel="alternate" type="text/html" title="Contribute: Join the Digital Library of Integral Ecology" /><published>2025-07-04T00:00:00+00:00</published><updated>2025-07-04T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/07/04/How%20you%20can%20help%20build%20and%20grow%20this%20open%20knowledge%20commons.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/07/04/How-you-can-help-build-and-grow-this-open-knowledge-commons"><![CDATA[<p>This project is not just about building technology — it’s about building <strong>community</strong>.</p>

<p>The Digital Library of Integral Ecology is a shared, open infrastructure for people who care about the Earth, its people, and our common future. It brings together knowledge from science, policy, spirituality, and grassroots voices — and we want you to be part of it.</p>

<hr />

<h2 id="ways-to-get-involved">Ways to Get Involved</h2>

<p>You don’t need to be a coder or a data scientist to contribute. Here are some ways anyone can help:</p>

<h3 id="annotate-reports">Annotate Reports</h3>
<p>Help tag ecological concepts, organizations, places, and ideas in documents using <a href="https://github.com/doccano/doccano">Doccano</a>. No technical skill required — just your careful reading.</p>

<h3 id="share-reports-and-sources">Share Reports and Sources</h3>
<p>We’re always looking for ecological reports in <strong>different languages</strong> and from <strong>diverse contexts</strong> (NGOs, indigenous communities, policy, faith-based orgs, etc.).</p>

<p>Send us PDFs or links to reports that should be included in the graph.</p>

<h3 id="train-the-ai">Train the AI</h3>
<p>If you’re technical, help us fine-tune our ecological NER models using annotated data. Or contribute to multilingual tagging tools and model evaluation.</p>

<h3 id="translate-and-extend">Translate and Extend</h3>
<p>Want to add new languages? New entity types like <strong>species</strong>, <strong>spiritual practices</strong>, or <strong>sacred sites</strong>? We’re building a framework that’s meant to grow with your contributions.</p>

<hr />

<h2 id="technical-contributors">Technical Contributors</h2>

<p>If you’re a developer, here’s where you can help:</p>

<ul>
  <li>Improve entity extraction and citation linking</li>
  <li>Enhance multilingual NLP support</li>
  <li>Automate ingestion from online archives</li>
  <li>Build interactive search and visualization tools</li>
  <li>Create public datasets from the graph (JSONL, CSV, RDF, etc.)</li>
</ul>

<p>Our <a href="https://github.com/clirdlf/dlie_knowledge_graph">GitHub repository</a> includes Docker-based workflows, NLP scripts, a Makefile, and blog documentation — all ready to clone and explore.</p>

<hr />

<h2 id="how-to-get-started">How to Get Started</h2>

<ol>
  <li>
    <p>Star or fork the project on GitHub: <a href="https://github.com/clirdlf/dlie_knowledge_graph">github.com/clirdlf/dlie_knowledge_graph</a></p>
  </li>
  <li>
    <p>Clone the repo:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   git clone https://github.com/clirdlf/dlie_knowledge_graph.git
   <span class="nb">cd </span>dlie_knowledge_graph
   make up
</code></pre></div></div>

<ol>
  <li>Try annotating or improving a model:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make pipeline <span class="nv">PDF</span><span class="o">=</span>my-report.pdf
</code></pre></div></div>

<ol>
  <li>Join us on discussions (coming soon)</li>
</ol>

<hr />

<h2 id="were-building-this-together">We’re Building This Together</h2>

<p>This project is inspired by the principles of <strong>integral ecology</strong>,  the idea that we must care for both the environment and the most vulnerable people it supports.</p>

<p>Building the Digital Library is a concrete act of hope. It’s a way to turn scattered, siloed knowledge into living, shared understanding.</p>

<p>Whether you’re a researcher, developer, librarian, artist, or student — you are welcome.</p>

<hr />

<blockquote>
  <p>“All it takes is one good person to restore hope.” — Pope Francis, Laudato Si’</p>
</blockquote>]]></content><author><name></name></author><summary type="html"><![CDATA[This project is not just about building technology — it’s about building community.]]></summary></entry><entry><title type="html">Training Our Own Ecological Language Model</title><link href="/dlie_knowledge_graph/2025/07/03/Turning-annotations-into-a-smarter,-ecology-specific-NER-system" rel="alternate" type="text/html" title="Training Our Own Ecological Language Model" /><published>2025-07-03T00:00:00+00:00</published><updated>2025-07-03T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/07/03/Turning%20annotations%20into%20a%20smarter,%20ecology-specific%20NER%20system.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/07/03/Turning-annotations-into-a-smarter,-ecology-specific-NER-system"><![CDATA[<h1 id="training-our-own-ecological-language-model">Training Our Own Ecological Language Model</h1>

<p>We’ve built a working NLP pipeline that tags organizations, locations, and ecological concepts from multilingual reports. We’ve even used Doccano to correct and improve those tags.</p>

<p>Now it’s time to <strong>teach the system</strong> to do better — by training our own custom NER model using the annotated data.</p>

<p>This post explains how we take human-labeled examples and turn them into a smarter, more accurate <strong>ecological language model</strong>.</p>

<hr />

<h2 id="why-train-a-custom-model">Why Train a Custom Model?</h2>

<p>spaCy’s built-in models are trained on general-purpose data. They work well, but they don’t understand:</p>

<ul>
  <li>Domain-specific terms like <em>“climate resilience”</em>, <em>“eco-spirituality”</em>, or <em>“integral development”</em></li>
  <li>New languages or spelling variants</li>
  <li>Cross-domain relationships common in integral ecology (science + ethics + economics)</li>
</ul>

<p>Training your own model lets you:</p>

<ul>
  <li>Add new entity types (e.g. <code class="language-plaintext highlighter-rouge">ECO_CONCEPT</code>)</li>
  <li>Improve accuracy for your documents</li>
  <li>Adapt the system to your language and context</li>
</ul>

<hr />

<h2 id="tools-we-use">Tools We Use</h2>

<p>We use <a href="https://spacy.io">spaCy</a>’s training framework. It supports:</p>

<ul>
  <li>Training from scratch or fine-tuning</li>
  <li>Evaluation metrics (precision, recall, F1)</li>
  <li>Easy use of exported Doccano data</li>
</ul>

<hr />

<h2 id="from-doccano-to-training-data">From Doccano to Training Data</h2>

<p>After annotating in Doccano:</p>

<ol>
  <li>Export your project as <code class="language-plaintext highlighter-rouge">.jsonl</code></li>
  <li>Use a converter to turn that into spaCy format:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spacy convert yourfile.jsonl ./train_data <span class="nt">--lang</span> en <span class="nt">--ner</span>
</code></pre></div></div>

<p>This gives you <code class="language-plaintext highlighter-rouge">.spacy</code> binary files for training.</p>

<p>Or use a Python script to convert to <code class="language-plaintext highlighter-rouge">train.json</code>, <code class="language-plaintext highlighter-rouge">dev.json</code> split manually.</p>

<p>⸻</p>

<h2 id="training-the-model">Training the Model</h2>

<ol>
  <li>Create a config file:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> spacy init config ./config.cfg <span class="nt">--lang</span> en <span class="nt">--pipeline</span> ner
</code></pre></div></div>

<ol>
  <li>Edit <code class="language-plaintext highlighter-rouge">config.cfg </code>to match your needs (entity labels, GPU, batch size, etc.)</li>
  <li>Prepare training assets:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> spacy init fill-config config.cfg config_final.cfg
</code></pre></div></div>

<ol>
  <li>Train the model:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> spacy train config_final.cfg <span class="nt">--output</span> ./output <span class="nt">--paths</span>.train ./train.spacy <span class="nt">--paths</span>.dev ./dev.spacy
</code></pre></div></div>

<p>When done, your trained model will be in:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./output/model-best
</code></pre></div></div>

<p>You can load it like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">output/model-best</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="measuring-progress">Measuring Progress</h2>

<p>During training, spaCy tracks:</p>

<ul>
  <li><strong>Precision</strong> — how many predicted entities were correct</li>
  <li><strong>Recall</strong> — how many true entities it found</li>
  <li><strong>F1 Score</strong> — a balance of the two</li>
</ul>

<p>This helps you compare:</p>

<ul>
  <li>Pre-trained model (baseline)</li>
  <li>Your fine-tuned model (specialized)</li>
</ul>

<p>⸻</p>

<h2 id="try-it-yourself">Try It Yourself</h2>

<p>Once you have annotations from Doccano:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make train-model
</code></pre></div></div>

<p>Or step through each stage with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> spacy init config ./config.cfg <span class="nt">--lang</span> en <span class="nt">--pipeline</span> ner
python <span class="nt">-m</span> spacy train config.cfg <span class="nt">--output</span> ./output <span class="nt">--paths</span>.train ./train.spacy <span class="nt">--paths</span>.dev ./dev.spacy
</code></pre></div></div>

<p>Test the result:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">output/model-best</span><span class="sh">"</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="nf">nlp</span><span class="p">(</span><span class="sh">"</span><span class="s">WWF works on climate resilience in the Amazon rainforest.</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">([(</span><span class="n">ent</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="n">ent</span><span class="p">.</span><span class="n">label_</span><span class="p">)</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">.</span><span class="n">ents</span><span class="p">])</span>
</code></pre></div></div>

<hr />

<h2 id="a-living-model">A Living Model</h2>

<p>As more reports are added and more annotations made, we can:</p>

<ul>
  <li>Retrain the model periodically</li>
  <li>Add multilingual support (e.g. <code class="language-plaintext highlighter-rouge">fr</code>, <code class="language-plaintext highlighter-rouge">es</code>, <code class="language-plaintext highlighter-rouge">zh</code>)</li>
  <li>Extend to more entity types (like <code class="language-plaintext highlighter-rouge">SPECIES</code>, <code class="language-plaintext highlighter-rouge">DOCUMENT</code>, <code class="language-plaintext highlighter-rouge">THEOLOGICAL_TERM</code>)</li>
</ul>

<p>This is how the Digital Library of Integral Ecology gets smarter over time — powered by human insight and collaborative learning.</p>

<p>⸻</p>

<h2 id="whats-next">What’s Next?</h2>

<p>In the final post, we’ll evaluate the full system and compare how well the pipeline performs <strong>before and after training</strong>, across languages and document types.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Training Our Own Ecological Language Model]]></summary></entry><entry><title type="html">Stitching the Graph: Saving Knowledge to Neo4j</title><link href="/dlie_knowledge_graph/2025/07/02/How-our-documents-and-entities-are-stored-and-visualized-as-a-graph" rel="alternate" type="text/html" title="Stitching the Graph: Saving Knowledge to Neo4j" /><published>2025-07-02T00:00:00+00:00</published><updated>2025-07-02T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/07/02/How%20our%20documents%20and%20entities%20are%20stored%20and%20visualized%20as%20a%20graph.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/07/02/How-our-documents-and-entities-are-stored-and-visualized-as-a-graph"><![CDATA[<p>So far, we’ve extracted clean text from ecological reports and used AI to identify important entities like organizations, places, and ecological concepts.</p>

<p>But identifying entities is only half the story. Now we need to <strong>connect</strong> them — to build our <strong>knowledge graph</strong>.</p>

<p>In this post, we show how we use <strong>Neo4j</strong>, a graph database, to stitch everything together into a network of knowledge.</p>

<hr />

<h2 id="what-is-a-graph-database">What is a Graph Database?</h2>

<p>A <strong>graph database</strong> stores information as <strong>nodes</strong> (things) and <strong>relationships</strong> (connections).</p>

<p>Unlike traditional databases, which store rows and columns, a graph lets you explore:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(WWF Report) –MENTIONS–&gt; (Amazon rainforest)
–CITES—–&gt; (IPBES 2019 Report)
–MENTIONS–&gt; (climate resilience)
</code></pre></div></div>

<p>Neo4j is the most popular open-source graph database. It’s:</p>
<ul>
  <li>Visual</li>
  <li>Easy to query (using a language called Cypher)</li>
  <li>Designed to handle relationships efficiently</li>
</ul>

<hr />

<h2 id="-what-we-store-in-the-graph">🧱 What We Store in the Graph</h2>

<p>Each document and entity we extract becomes a <strong>node</strong>, and their connections are modeled as <strong>relationships</strong>.</p>

<h3 id="node-types">Node Types</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">:Document</code> — the report itself</li>
  <li><code class="language-plaintext highlighter-rouge">:Entity</code> — an extracted item (e.g. “WWF”, “Amazon rainforest”)</li>
</ul>

<p>Each node has properties like:</p>

<div class="language-cypher highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ss">(</span><span class="nc">:Document</span> <span class="ss">{</span><span class="py">name:</span> <span class="s2">"wwf_amazon_report"</span><span class="ss">,</span> <span class="py">lang:</span> <span class="s2">"en"</span><span class="ss">})</span>
<span class="ss">(</span><span class="nc">:Entity</span> <span class="ss">{</span><span class="py">text:</span> <span class="s2">"climate resilience"</span><span class="ss">,</span> <span class="py">label:</span> <span class="s2">"ECO_CONCEPT"</span><span class="ss">})</span>
</code></pre></div></div>

<h3 id="relationship-types">Relationship Types</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">(:Document)-[:MENTIONS]-&gt;(:Entity)</code></li>
  <li><code class="language-plaintext highlighter-rouge">(:Document)-[:CITES]-&gt;(:Document)</code> (coming soon via citation parsing)</li>
</ul>

<p>These relationships make the knowledge <strong>navigable</strong>, not just searchable.</p>

<h2 id="how-it-works-in-code">How It Works in Code</h2>

<p>After entity tagging, we load the data into Neo4j using:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python graph_upload.py my-report.entities.json
</code></pre></div></div>

<p>This script:</p>

<ol>
  <li>Loads the entity JSON file</li>
  <li>Creates the <code class="language-plaintext highlighter-rouge">:Document</code> node</li>
  <li>Creates one <code class="language-plaintext highlighter-rouge">:Entity</code> node per unique mention</li>
  <li>Connects them with <code class="language-plaintext highlighter-rouge">:MENTIONS</code> relationships</li>
</ol>

<p>You can view this graph in Neo4j’s browser:</p>

<p><a href="http://localhost:7474">http://localhost:7474</a></p>

<p>Use the default login:</p>

<ul>
  <li><strong>Username:</strong> neo4j</li>
  <li><strong>Password:</strong> password</li>
</ul>

<p>And run a query like:</p>

<div class="language-cypher highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">MATCH</span><span class="w"> </span><span class="ss">(</span><span class="py">d:</span><span class="n">Document</span><span class="ss">)</span><span class="o">-</span><span class="ss">[</span><span class="nc">:MENTIONS</span><span class="ss">]</span><span class="o">-&gt;</span><span class="ss">(</span><span class="py">e:</span><span class="n">Entity</span><span class="ss">)</span>
<span class="k">RETURN</span> <span class="n">d</span><span class="ss">,</span> <span class="n">e</span>
</code></pre></div></div>

<hr />

<h2 id="try-it-yourself">Try It Yourself</h2>

<p>If you’ve run the full pipeline:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make pipeline <span class="nv">PDF</span><span class="o">=</span>my-report.pdf
</code></pre></div></div>

<p>The upload to Neo4j is done automatically. Otherwise, you can run it directly:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose <span class="nb">exec </span>worker python graph_upload.py my-report.entities.json
</code></pre></div></div>

<p>Then open Neo4j in your browser and start exploring the graph!</p>

<hr />

<h2 id="what-can-you-do-with-it">What Can You Do With It?</h2>

<p>Once in the graph, you can:</p>

<ul>
  <li>Find all documents mentioning a specific concept</li>
  <li>Discover what organizations work in similar regions</li>
  <li>Visualize themes and citations across languages</li>
  <li>Prepare data for annotation or deeper AI training</li>
</ul>

<p>This is where our static data becomes <strong>living knowledge</strong>.</p>

<hr />

<h2 id="whats-next">What’s Next?</h2>

<p>Now that our knowledge is structured and searchable, we’ll look at how to <strong>export the data to Doccano</strong>, a simple annotation tool that lets humans teach the system to improve over time.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[So far, we’ve extracted clean text from ecological reports and used AI to identify important entities like organizations, places, and ecological concepts.]]></summary></entry><entry><title type="html">Evaluating the Results: Accuracy, Speed, and Insight</title><link href="/dlie_knowledge_graph/2025/07/01/Measuring-how-well-our-models-work,-and-what-they-help-us-uncover" rel="alternate" type="text/html" title="Evaluating the Results: Accuracy, Speed, and Insight" /><published>2025-07-01T00:00:00+00:00</published><updated>2025-07-01T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/07/01/Measuring%20how%20well%20our%20models%20work,%20and%20what%20they%20help%20us%20uncover.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/07/01/Measuring-how-well-our-models-work,-and-what-they-help-us-uncover"><![CDATA[<p>We’ve come a long way — from PDFs to graphs, from automatic tagging to human annotation, and finally to training our own ecological NLP model.</p>

<p>But how do we know if it’s working?</p>

<p>In this post, we’ll explore how to evaluate your pipeline and model using both <strong>standard NLP metrics</strong> and <strong>real-world ecological insight</strong>.</p>

<hr />

<h2 id="why-evaluate">Why Evaluate?</h2>

<p>Evaluation helps us understand:</p>

<ul>
  <li>✅ Is the model tagging entities accurately?</li>
  <li>✅ Does it work well across languages?</li>
  <li>✅ Is it fast enough for real-world use?</li>
  <li>✅ What kinds of mistakes does it make?</li>
</ul>

<p>Evaluation isn’t just about numbers — it’s about improving <strong>trust</strong>, <strong>transparency</strong>, and <strong>impact</strong>.</p>

<hr />

<h2 id="key-metrics">Key Metrics</h2>

<h3 id="precision">Precision</h3>
<p>The % of predicted entities that were correct</p>
<blockquote>
  <p>Did it predict something meaningful?</p>
</blockquote>

<h3 id="recall">Recall</h3>
<p>The % of correct entities that were successfully predicted</p>
<blockquote>
  <p>Did it miss important things?</p>
</blockquote>

<h3 id="f1-score">F1 Score</h3>
<p>The balance of precision and recall</p>
<blockquote>
  <p>A good all-around indicator</p>
</blockquote>

<p>These metrics are calculated by comparing your model’s output to <strong>human-annotated data</strong>, usually exported from Doccano.</p>

<hr />

<h2 id="how-to-evaluate-with-spacy">How to Evaluate with spaCy</h2>

<p>After training, spaCy automatically evaluates on your dev/test set and shows:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>✔ Accuracy: 89.2%
✔ Precision: 88.5%
✔ Recall: 90.1%
✔ F1 Score: 89.3
</code></pre></div></div>

<p>You can also evaluate manually using:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> spacy evaluate ./output/model-best ./dev.spacy
</code></pre></div></div>

<p>Or export annotated JSONL and compare predictions programmatically.</p>

<h2 id="speed-and-runtime">Speed and Runtime</h2>

<p>You can also evaluate <strong>how fast</strong> your model runs:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">spacy</span><span class="p">,</span> <span class="n">time</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">output/model-best</span><span class="sh">"</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">The WWF is active in the Amazon on climate resilience.</span><span class="sh">"</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">doc</span> <span class="o">=</span> <span class="nf">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nf">print</span><span class="p">([(</span><span class="n">ent</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="n">ent</span><span class="p">.</span><span class="n">label_</span><span class="p">)</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">.</span><span class="n">ents</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Time:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="sh">"</span><span class="s">seconds</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>This helps you decide:</p>

<ul>
  <li>Which model is best for live pipelines vs. batch runs</li>
  <li>When to use sm vs. <code class="language-plaintext highlighter-rouge">lg</code> or <code class="language-plaintext highlighter-rouge">trf</code> models</li>
</ul>

<hr />

<h2 id="beyond-accuracy-ecological-insight">Beyond Accuracy: Ecological Insight</h2>

<p>Numbers matter, but so does <strong>usefulness</strong>!</p>

<p>Ask:</p>

<ul>
  <li>Does the graph help us find new connections?</li>
  <li>Are multilingual documents being fairly represented?</li>
  <li>Does it highlight underrepresented organizations or regions?</li>
  <li>Is it surfacing unexpected insights for researchers or communities?</li>
</ul>

<p>These qualitative measures can be just as valuable as F1 score.</p>

<h2 id="try-it-yourself">Try It Yourself</h2>

<p>After training, run:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> spacy evaluate output/model-best dev.spacy
</code></pre></div></div>

<p>Or test it interactively:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">output/model-best</span><span class="sh">"</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="nf">nlp</span><span class="p">(</span><span class="sh">"</span><span class="s">UNEP published a report on ecosystem resilience in East Africa.</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">([(</span><span class="n">ent</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="n">ent</span><span class="p">.</span><span class="n">label_</span><span class="p">)</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">.</span><span class="n">ents</span><span class="p">])</span>
</code></pre></div></div>

<p>Then compare the results to the baseline model (<code class="language-plaintext highlighter-rouge">en_core_web_sm</code>) or TaxoNERD.</p>

<hr />

<h2 id="whats-next">What’s Next</h2>

<p>You’ve now seen the full lifecycle:</p>

<ol>
  <li>Extract text and citations from reports</li>
  <li>Tag entities across languages</li>
  <li>Upload to a knowledge graph</li>
  <li>Annotate and refine human feedback</li>
  <li>Train and evaluate your own model</li>
</ol>

<p>You’re ready to contribute — or build your own ecological pipeline.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[We’ve come a long way — from PDFs to graphs, from automatic tagging to human annotation, and finally to training our own ecological NLP model.]]></summary></entry><entry><title type="html">Annotation: Teaching the System to Be Smarter</title><link href="/dlie_knowledge_graph/2025/06/29/A-look-at-Doccano-and-how-humans-refine-machine-learning" rel="alternate" type="text/html" title="Annotation: Teaching the System to Be Smarter" /><published>2025-06-29T00:00:00+00:00</published><updated>2025-06-29T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/06/29/A%20look%20at%20Doccano%20and%20how%20humans%20refine%20machine%20learning.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/06/29/A-look-at-Doccano-and-how-humans-refine-machine-learning"><![CDATA[<p>Our pipeline can already extract text from reports, tag entities, and build a multilingual knowledge graph — but how accurate is it?</p>

<p>The truth is: even good AI needs <strong>human correction</strong>. That’s where <strong>annotation</strong> comes in.</p>

<p>In this post, we’ll show you how we use <strong>Doccano</strong>, a friendly web interface, to correct the output of our pipeline and help the model improve over time.</p>

<hr />

<h2 id="why-annotation-matters">Why Annotation Matters</h2>

<p>Even the best models make mistakes:</p>

<ul>
  <li>Misclassifying entities (e.g. “Amazon” as a product instead of a forest)</li>
  <li>Missing subtle ecological terms (like “resilience” or “eco-conversion”)</li>
  <li>Struggling with languages like Arabic or complex phrases</li>
</ul>

<p>Annotation lets humans:</p>

<ul>
  <li>Correct the labels</li>
  <li>Add missing terms</li>
  <li>Build reliable training data</li>
</ul>

<p>It’s like proofreading — but for a machine learning system.</p>

<hr />

<h2 id="what-is-doccano">What is Doccano?</h2>

<p><a href="https://doccano.github.io/doccano/">Doccano</a> is an open-source tool for <strong>annotating text for NLP tasks</strong>.</p>

<p>It provides:</p>

<ul>
  <li>A browser-based interface</li>
  <li>Support for named entity recognition (NER), classification, translation, and more</li>
  <li>Role-based user access</li>
  <li>Easy data import and export</li>
</ul>

<p>We use Doccano to refine the results of <code class="language-plaintext highlighter-rouge">ner_pipeline.py</code>.</p>

<hr />

<h2 id="exporting-to-doccano-format">Exporting to Doccano Format</h2>

<p>After entity tagging, we export the results in <a href="https://github.com/doccano/doccano/blob/master/docs/api.md#annotation-format">JSONL format</a>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python export_doccano.py my-report.entities.json
</code></pre></div></div>

<p>Which creates:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"The WWF report on the Amazon rainforest..."</span><span class="p">,</span><span class="w">
  </span><span class="nl">"labels"</span><span class="p">:</span><span class="w"> </span><span class="p">[[</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="s2">"ORG"</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="mi">20</span><span class="p">,</span><span class="w"> </span><span class="mi">36</span><span class="p">,</span><span class="w"> </span><span class="s2">"LOC"</span><span class="p">]]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>This format can be imported directly into Doccano.</p>

<hr />

<h2 id="using-doccano-locally">Using Doccano Locally</h2>

<p>If you’re running the project with Docker Compose, Doccano is already available at:</p>

<p><a href="http://localhost:8000">http://localhost:8000</a></p>

<p>Log in with</p>

<ul>
  <li><strong>Username:</strong> admin</li>
  <li><strong>Password:</strong> admin</li>
</ul>

<p>From there, you can</p>

<ol>
  <li>Create a new NER project</li>
  <li>Import your .jsonl file (generated in previous step)</li>
  <li>Start tagging!</li>
</ol>

<hr />

<h2 id="the-feedback-loop">The Feedback Loop</h2>

<p>Once documents are annotated in Doccano, we:</p>

<ol>
  <li>Export the clean annotations</li>
  <li>Convert them to spaCy training format</li>
  <li>Fine-tune a custom NER model</li>
</ol>

<p>This cycle helps the system:</p>

<ul>
  <li>Improve tagging accuracy</li>
  <li>Recognize new or uncommon terms</li>
  <li>Adapt to multilingual and ecological contexts</li>
</ul>

<p>We call this process <strong>active learning</strong>.</p>

<hr />

<h2 id="try-it-yourself">Try It Yourself</h2>

<p>If you’ve already run:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make pipeline <span class="nv">PDF</span><span class="o">=</span>my-report.pdf
</code></pre></div></div>

<p>Then a Doccano file will be created at:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data/doccano/my-report.entities.jsonl
</code></pre></div></div>

<p>Import this file into your Doccano project and try refining the labels!</p>

<h2 id="who-can-help">Who Can Help?</h2>

<p>Annotation is one of the best ways to contribute, especially if you:</p>

<ul>
  <li>Are a researcher in ecology, theology, or social sciences</li>
  <li>Are bilingual or multilingual</li>
  <li>Want to help shape AI to better understand the world</li>
</ul>

<p>All you need is careful attention. <strong>No coding required!</strong></p>

<h2 id="whats-next">What’s Next?</h2>

<p>Now that we have clean, annotated data, we’re ready to train our own <strong>ecologically informed NER model</strong>.</p>

<p>In the next post, we’ll walk through how to train a custom spaCy pipeline using your Doccano data.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Our pipeline can already extract text from reports, tag entities, and build a multilingual knowledge graph — but how accurate is it?]]></summary></entry><entry><title type="html">Tagging the World: Finding Places, Plants, and Ideas with AI</title><link href="/dlie_knowledge_graph/2025/06/28/How-NLP-tools-identify-ecological-concepts,-organizations,-and-more" rel="alternate" type="text/html" title="Tagging the World: Finding Places, Plants, and Ideas with AI" /><published>2025-06-28T00:00:00+00:00</published><updated>2025-06-28T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/06/28/How%20NLP%20tools%20identify%20ecological%20concepts,%20organizations,%20and%20more.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/06/28/How-NLP-tools-identify-ecological-concepts,-organizations,-and-more"><![CDATA[<p>Once we’ve extracted clean text from a PDF, the next step is to <strong>understand what’s being talked about</strong>.</p>

<p>That’s where Natural Language Processing (NLP) comes in.</p>

<p>We use NLP to scan the text and find key pieces of information — like:</p>

<ul>
  <li>Locations (e.g. “Amazon rainforest”)</li>
  <li>Organizations (e.g. “WWF”)</li>
  <li>Ecological concepts (e.g. “resilience”, “biodiversity loss”)</li>
  <li>Citations (e.g. “IPBES 2019 Report”)</li>
</ul>

<p>Each of these is called an <strong>entity</strong>, and this process is called <strong>Named Entity Recognition (NER)</strong>.</p>

<hr />

<h2 id="what-is-named-entity-recognition">What is Named Entity Recognition?</h2>

<p>NER is a type of AI model that reads text and labels the parts that represent real-world things.</p>

<p>Example:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original text:
The WWF report on the Amazon rainforest highlights climate resilience strategies.

NER output:
[ORG: WWF], [LOC: Amazon rainforest], [ECO_CONCEPT: climate resilience]
</code></pre></div></div>

<p>This gives us structured data from unstructured sentences, and helps us populate our knowledge graph with <strong>nodes and connections</strong>.</p>

<hr />

<h2 id="tools-we-use">Tools We Use</h2>

<h3 id="spacy">spaCy</h3>

<p>We use spaCy, a popular open-source NLP library that can:</p>

<ul>
  <li>Work in English, Spanish, French, Chinese, Russian, and more</li>
  <li>Recognize standard entities like ORG, LOC, PERSON, etc.</li>
  <li>Run fast and integrate easily with Python</li>
</ul>

<h3 id="taxonerd">TaxoNERD</h3>

<p>For ecological texts, general NLP isn’t enough — so we also use TaxoNERD, a tool trained to detect ecological and taxonomic entities, like:</p>

<ul>
  <li>Ecosystem types</li>
  <li>Species groups</li>
  <li>Environmental terms</li>
</ul>

<p>TaxoNERD uses a model called BioBERT and is specialized for ecological language.</p>

<h3 id="multilingual-support">Multilingual Support</h3>

<p>We also include spaCy models for:</p>

<ul>
  <li>fr_core_news_lg (French)</li>
  <li>es_core_news_lg (Spanish)</li>
  <li>zh_core_web_trf (Chinese)</li>
  <li>xx_ent_wiki_sm (basic multilingual)</li>
</ul>

<p>This makes the system l<strong>anguage-aware</strong>, even when documents span continents.</p>

<hr />

<h2 id="how-it-works-in-code">How It Works in Code</h2>

<p>The tagging is handled by this command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python ner_pipeline.py my-report.txt
</code></pre></div></div>

<p>Which produces:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"..."</span><span class="p">,</span><span class="w">
  </span><span class="nl">"entities"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="nl">"start"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="nl">"end"</span><span class="p">:</span><span class="w"> </span><span class="mi">22</span><span class="p">,</span><span class="w"> </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"LOC"</span><span class="p">,</span><span class="w"> </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Amazon rainforest"</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"start"</span><span class="p">:</span><span class="w"> </span><span class="mi">31</span><span class="p">,</span><span class="w"> </span><span class="nl">"end"</span><span class="p">:</span><span class="w"> </span><span class="mi">34</span><span class="p">,</span><span class="w"> </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ORG"</span><span class="p">,</span><span class="w"> </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"WWF"</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"start"</span><span class="p">:</span><span class="w"> </span><span class="mi">45</span><span class="p">,</span><span class="w"> </span><span class="nl">"end"</span><span class="p">:</span><span class="w"> </span><span class="mi">63</span><span class="p">,</span><span class="w"> </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ECO_CONCEPT"</span><span class="p">,</span><span class="w"> </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"climate resilience"</span><span class="p">}</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>This is saved as <code class="language-plaintext highlighter-rouge">my-report.entities.json</code> in the <code class="language-plaintext highlighter-rouge">data/output/</code> folder.</p>

<hr />

<h2 id="why-this-matters">Why This Matters</h2>

<p>Recognizing entities allows us to:</p>
<ul>
  <li>Link a sentence to the right concepts</li>
  <li>Group reports by theme or region</li>
  <li>Connect related documents, even across languages</li>
  <li>Support annotation and model training</li>
</ul>

<p>It’s the first step toward turning plain text into a semantic map.</p>

<hr />

<h2 id="try-it-yourself">Try It Yourself</h2>

<p>If you’ve already extracted text using:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make pipeline <span class="nv">PDF</span><span class="o">=</span>my-report.pdf
</code></pre></div></div>

<p>The entity tagging will run automatically. Check the output in:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data/output/my-report.entities.json
</code></pre></div></div>

<p>You can also run it independently:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose <span class="nb">exec </span>worker python ner_pipeline.py my-report.txt
</code></pre></div></div>

<hr />

<h2 id="whats-next">What’s Next?</h2>

<p>Next, we’ll take these entities and load them into Neo4j — our graph database — where we can start to visualize and query relationships.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Once we’ve extracted clean text from a PDF, the next step is to understand what’s being talked about.]]></summary></entry><entry><title type="html">From PDF to Text: Extracting Meaning from Documents</title><link href="/dlie_knowledge_graph/2025/06/27/Learn-how-we-pull-clean,-useful-text-from-messy-PDFs-and-scientific-citations" rel="alternate" type="text/html" title="From PDF to Text: Extracting Meaning from Documents" /><published>2025-06-27T00:00:00+00:00</published><updated>2025-06-27T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/06/27/Learn%20how%20we%20pull%20clean,%20useful%20text%20from%20messy%20PDFs%20and%20scientific%20citations.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/06/27/Learn-how-we-pull-clean,-useful-text-from-messy-PDFs-and-scientific-citations"><![CDATA[<p>The first step in building a knowledge graph for integral ecology is simple in concept — but tricky in practice:</p>

<blockquote>
  <p>How do we get clean, usable <strong>text</strong> from messy, multilingual <strong>PDF reports</strong>?</p>
</blockquote>

<p>This post explains how we extract both the <strong>raw text</strong> and the <strong>structured citations</strong> from reports using two tools:</p>
<ul>
  <li>PyMuPDF for text</li>
  <li>GROBID for citations and metadata</li>
</ul>

<hr />

<h2 id="why-this-matters">Why This Matters</h2>

<p>PDFs are designed for printing, not for reading by machines.</p>

<p>They can include:</p>
<ul>
  <li>Columns and footnotes</li>
  <li>Images, tables, and scanned pages</li>
  <li>Embedded fonts or malformed characters</li>
  <li>Multiple languages in one document</li>
</ul>

<p>If we want to detect entities and link knowledge later, we need high-quality <strong>plain text</strong>.</p>

<hr />

<h2 id="step-1-extract-text-with-pymupdf">Step 1: Extract Text with PyMuPDF</h2>

<p>We use <a href="https://pymupdf.readthedocs.io/"><code class="language-plaintext highlighter-rouge">PyMuPDF</code></a> (also known as <code class="language-plaintext highlighter-rouge">fitz</code>) to extract the text page-by-page from a PDF.</p>

<p>It:</p>
<ul>
  <li>Preserves layout well</li>
  <li>Handles multiple languages</li>
  <li>Works with scanned+OCR’d documents if text is embedded</li>
</ul>

<p><strong>Example output:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Amazon rainforest is shrinking rapidly.

WWF reported that deforestation increased 12% in 2023.
</code></pre></div></div>

<p>This text gets saved as <code class="language-plaintext highlighter-rouge">report.txt</code>.</p>

<h2 id="step-2-extract-citations-with-grobid">Step 2: Extract Citations with GROBID</h2>

<p>Next, we use <a href="https://github.com/kermitt2/grobid">GROBID</a>, a tool that reads the bibliography and metadata of academic papers and reports.</p>

<p>GROBID converts messy citation lists like:</p>

<blockquote>
  <p>[12] Smith, J., “Biodiversity and Forests”, Nature, 2020</p>
</blockquote>

<p>Into structured, machine-readable <strong>TEI XML</strong>, which can include:</p>

<ul>
  <li>Title</li>
  <li>Authors</li>
  <li>Year</li>
  <li>Journal or publisher</li>
  <li>DOI or identifiers</li>
</ul>

<p>We save this as <code class="language-plaintext highlighter-rouge">report.biblio.xml</code>.</p>

<p>Later in the pipeline, this will help us:</p>

<ul>
  <li>Build <strong>:CITES relationships</strong> in the graph</li>
  <li>Match references across reports</li>
  <li>Cluster similar reports by their sources</li>
</ul>

<h2 id="how-this-works-in-code">How This Works in Code</h2>

<p>Our system includes a script that does this automatically:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python extract_text.py my-report.pdf
</code></pre></div></div>

<p>It will:</p>

<ol>
  <li>Extract the full text using PyMuPDF → <code class="language-plaintext highlighter-rouge">my-report.txt</code></li>
  <li>Send the PDF to the GROBID API → <code class="language-plaintext highlighter-rouge">my-report.biblio.xml</code></li>
</ol>

<p>The script runs inside a Docker container and outputs files to the <code class="language-plaintext highlighter-rouge">/data/output/</code> folder.</p>

<h2 id="try-it-yourself">Try It Yourself</h2>

<p>If you’ve followed the setup from <a href="/posts/20_building_blocks/">Part 2</a>, you can run:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make pipeline PDF=my-report.pdf
</code></pre></div></div>

<p>This will:</p>

<ul>
  <li>Extract text and citations</li>
  <li>Tag entities (coming up in Part 4)</li>
  <li>Load data into Neo4j</li>
  <li>Export annotated text for review</li>
</ul>

<p>Check the results in <code class="language-plaintext highlighter-rouge">data/output/</code> — you’ll see <code class="language-plaintext highlighter-rouge">.txt</code>, <code class="language-plaintext highlighter-rouge">.entities.json</code>, and <code class="language-plaintext highlighter-rouge">.biblio.xml</code> files.</p>

<hr />

<h2 id="what-we-learned">What We Learned</h2>

<ul>
  <li>PDFs are tricky, but PyMuPDF gives us reliable plain text</li>
  <li>GROBID gives us structured citations, ready for linking</li>
  <li>Clean text is the foundation for everything that follows</li>
</ul>

<hr />

<h2 id="whats-next">What’s Next?</h2>

<p>In the next post, we’ll explore how we tag entities in the text using AI — recognizing organizations, locations, ecological concepts, and more.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[The first step in building a knowledge graph for integral ecology is simple in concept — but tricky in practice:]]></summary></entry><entry><title type="html">Building Blocks: Documents, Entities, and Relationships</title><link href="/dlie_knowledge_graph/2025/06/26/How-reports-are-transformed-into-structured,-searchable-networks-of-knowledge" rel="alternate" type="text/html" title="Building Blocks: Documents, Entities, and Relationships" /><published>2025-06-26T00:00:00+00:00</published><updated>2025-06-26T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/06/26/How%20reports%20are%20transformed%20into%20structured,%20searchable%20networks%20of%20knowledge.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/06/26/How-reports-are-transformed-into-structured,-searchable-networks-of-knowledge"><![CDATA[<p>In our last post, we introduced the vision: building a knowledge graph for <strong>integral ecology</strong>, a way to connect people, places, organizations, and ideas across languages and disciplines.</p>

<p>But how exactly do we turn messy PDFs and complex reports into a meaningful, searchable web of knowledge?</p>

<p>We start with three key building blocks:</p>

<hr />

<h2 id="1-documents">1. Documents</h2>

<p>The foundation of our knowledge graph is a <strong>document</strong> — a report, academic paper, policy brief, or even a faith-based reflection.</p>

<p>Each document is:</p>

<ul>
  <li>A single file (usually a PDF)</li>
  <li>With a title, language, source (UNEP, WWF, OpenAlex, etc.)</li>
  <li>That contains many sentences — and lots of <strong>information hidden in plain text</strong></li>
</ul>

<p>We treat each document as a <strong>node</strong> in the graph, and from there, we extract meaning.</p>

<hr />

<h2 id="2-entities">2. Entities</h2>

<p>Entities are <strong>things that the document talks about</strong> — people, organizations, species, places, and ecological concepts.</p>

<p>For example:</p>

<table>
  <thead>
    <tr>
      <th>Entity Text</th>
      <th>Label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Amazon rainforest</td>
      <td><code class="language-plaintext highlighter-rouge">LOCATION</code></td>
    </tr>
    <tr>
      <td>WWF</td>
      <td><code class="language-plaintext highlighter-rouge">ORG</code></td>
    </tr>
    <tr>
      <td>climate resilience</td>
      <td><code class="language-plaintext highlighter-rouge">ECO_CONCEPT</code></td>
    </tr>
    <tr>
      <td>Laudato Si’</td>
      <td><code class="language-plaintext highlighter-rouge">DOCUMENT</code></td>
    </tr>
  </tbody>
</table>

<p>Our system uses <strong>Natural Language Processing (NLP)</strong> tools to automatically recognize these entities in many languages — with models trained on large text collections.</p>

<p>Later, we’ll even <strong>fine-tune our own models</strong> to be more accurate for ecological language.</p>

<hr />

<h2 id="3-relationships">3. Relationships</h2>

<p>The real power of a knowledge graph comes from the <strong>connections between entities</strong> — also called <strong>edges</strong> or <strong>relationships</strong>.</p>

<p>Some examples:</p>

<ul>
  <li>A document <strong>MENTIONS</strong> an entity</li>
  <li>A document <strong>CITES</strong> another document</li>
  <li>A concept <strong>IS_RELATED_TO</strong> another concept</li>
  <li>An organization <strong>WORKS_IN</strong> a specific region</li>
</ul>

<p>These relationships turn isolated data points into an <strong>interconnected network</strong> — where you can explore patterns, paths, and shared meaning.</p>

<hr />

<h2 id="putting-it-together">Putting It Together</h2>

<p>Here’s a simple example:</p>

<div class="language-cypher highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ss">[</span><span class="n">WWF</span> <span class="n">Report</span><span class="ss">]</span><span class="err">–</span><span class="n">MENTIONS</span><span class="err">–</span><span class="o">&gt;</span><span class="ss">[</span><span class="n">Amazon</span> <span class="n">rainforest</span><span class="ss">]</span>
<span class="err">–</span><span class="n">MENTIONS</span><span class="err">–</span><span class="o">&gt;</span><span class="ss">[</span><span class="n">climate</span> <span class="n">resilience</span><span class="ss">]</span>
<span class="err">–</span><span class="n">CITES</span><span class="err">—–</span><span class="o">&gt;</span><span class="ss">[</span><span class="n">IPBES</span> <span class="mi">2022</span> <span class="n">Report</span><span class="ss">]</span>
</code></pre></div></div>

<p>In the graph database, each of these is a <strong>node</strong> (document or entity) and each arrow is a <strong>relationship</strong>.</p>

<p>We can now:</p>

<ul>
  <li>Search for all reports mentioning “climate resilience”</li>
  <li>Find which NGOs cite a particular scientific assessment</li>
  <li>Map ecological priorities across language and region</li>
</ul>

<hr />

<h2 id="why-it-matters">Why It Matters</h2>

<p>This model gives us:</p>

<ul>
  <li><strong>Structure</strong> — so we can search and analyze consistently</li>
  <li><strong>Scalability</strong> — works for hundreds or thousands of documents</li>
  <li><strong>Interoperability</strong> — can be visualized, queried, and shared</li>
</ul>

<p>And it sets the stage for automation, collaboration, and learning.</p>

<hr />

<h2 id="try-it-yourself-clone--run-the-project">Try It Yourself: Clone &amp; Run the Project</h2>

<p>You can explore and run this pipeline locally using Docker.</p>

<h3 id="prerequisites">Prerequisites</h3>

<ul>
  <li><a href="https://www.docker.com/">Docker</a> (Desktop or CLI)</li>
  <li><a href="https://git-scm.com/">Git</a></li>
</ul>

<h3 id="step-1-clone-the-repository">Step 1: Clone the Repository</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/clirdlf/dlie_knowledge_graph.git
<span class="nb">cd </span>dlie_knowledge_graph
</code></pre></div></div>

<h3 id="step-2-build-and-start-the-system">Step 2: Build and Start the System</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make build
make up
</code></pre></div></div>

<p>This will spin up:</p>

<ul>
  <li><a href="https://grobid.readthedocs.io/en/latest/">GROBID</a> for citation parsing</li>
  <li><a href="https://neo4j.com/">Neo4j</a> for the knowledge graph</li>
  <li><a href="https://doccano.github.io/doccano/">Doccano</a> for annotation</li>
  <li>A Python environment for text and entity extraction</li>
</ul>

<p>Then, you can run the full pipeline like this:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make pipeline <span class="nv">PDF</span><span class="o">=</span>/data/input/202206_IPBES_GLOBALREPORT.pdf
</code></pre></div></div>

<p>You’ll find the results in the <code class="language-plaintext highlighter-rouge">data/output/</code> and <code class="language-plaintext highlighter-rouge">data/doccano/</code> folders.</p>

<h2 id="whats-next">What’s Next?</h2>

<p>In the next post, we’ll start <strong>extracting text from real reports</strong> — even messy PDFs — using smart tools like PyMuPDF and GROBID.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In our last post, we introduced the vision: building a knowledge graph for integral ecology, a way to connect people, places, organizations, and ideas across languages and disciplines.]]></summary></entry><entry><title type="html">What is a Knowledge Graph for Integral Ecology?</title><link href="/dlie_knowledge_graph/2025/06/25/Introduction-to-what-we're-building-and-why-it-matters" rel="alternate" type="text/html" title="What is a Knowledge Graph for Integral Ecology?" /><published>2025-06-25T00:00:00+00:00</published><updated>2025-06-25T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/06/25/Introduction%20to%20what%20we&apos;re%20building%20and%20why%20it%20matters.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/06/25/Introduction-to-what-we&apos;re-building-and-why-it-matters"><![CDATA[<p>We live in an age of overwhelming information, but ecological knowledge, wisdom, and action often remain <strong>fragmented</strong>, buried in reports, scattered across languages, or locked in formats only specialists can access.</p>

<p>That’s where the <strong>Digital Library of Integral Ecology</strong> comes in. Our mission is to bring this information together, from scientific papers to NGO reports to faith-based reflections, into a single, interconnected digital library that helps researchers, educators, and communities act for the common good. One of the tools that helps us discovery and interrogate integral ecology is a knowledge graph. This codebase and set of tutorials will walk you through the technologies, opportunities, and tradeoffs in building this feature of the Digitial Library of Integral Ecology.</p>

<hr />

<h2 id="what-do-we-mean-by-integral-ecology">What Do We Mean by “Integral Ecology”?</h2>

<p>The term <em>integral ecology</em> comes from <em><a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a></em>, Pope Francis’ encyclical on the environment. It’s about recognizing the <strong>deep connections between ecological, social, cultural, and spiritual concerns</strong>. Climate change, deforestation, loss of biodiversity — these aren’t just technical problems. They are moral ones, economic ones, and spiritual ones too.</p>

<p>Integral ecology asks us to <strong>think in systems</strong>, and to see how everything is connected.</p>

<hr />

<h2 id="and-whats-a-knowledge-graph">And What’s a Knowledge Graph?</h2>

<p>A <strong>knowledge graph</strong> is a way of storing and exploring knowledge by looking at <strong>relationships</strong>. Imagine a big web:</p>

<ul>
  <li>A report mentions <strong>“Amazon rainforest”</strong></li>
  <li>It connects to a <strong>location</strong></li>
  <li>The report is published by <strong>UNESCO</strong></li>
  <li>It discusses concepts like <strong>resilience</strong> and <strong>biodiversity</strong></li>
  <li>It cites other documents that are connected too</li>
</ul>

<p>All of this is represented not just as flat text, but as <strong>linked data</strong> — relationships between concepts, people, places, and ideas. This is what lets us ask better questions and see deeper patterns.</p>

<hr />

<h2 id="what-were-building">What We’re Building</h2>

<p>We are creating a system that:</p>

<ol>
  <li><strong>Extracts text</strong> from ecological reports and scientific papers (even in PDF format)</li>
  <li><strong>Identifies key concepts</strong>, organizations, places, species, and ideas — in multiple languages</li>
  <li><strong>Links them together</strong> in a searchable, visual graph (using Neo4j)</li>
  <li><strong>Lets people annotate</strong> and refine that knowledge with simple tools</li>
  <li><strong>Trains smarter AI models</strong> over time that understand ecology more deeply</li>
</ol>

<hr />

<h2 id="why-it-matters">Why It Matters</h2>

<ul>
  <li>Researchers can discover connections across disciplines and languages</li>
  <li>NGOs can map their work to broader systems and goals</li>
  <li>Educators can explore real-world ecological examples interactively</li>
  <li>Communities can build shared understanding of their bioregions</li>
</ul>

<p>We believe this is a project not just of technology — but of <strong>ecological conversion</strong>.</p>

<hr />

<h2 id="whats-next">What’s Next?</h2>

<p>In the next post, we’ll walk through the building blocks of the graph: <strong>documents, entities, and relationships</strong>, and how we transform text into structure.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[We live in an age of overwhelming information, but ecological knowledge, wisdom, and action often remain fragmented, buried in reports, scattered across languages, or locked in formats only specialists can access.]]></summary></entry></feed>