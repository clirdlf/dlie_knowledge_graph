<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>Digitial Library of Integral Ecology Knowledge Graph</title>
  <subtitle>Tutorials for building open tools for ecological understanding</subtitle>
  <link href="https://clirdlf.github.io/dlie_knowledge_graph/feed.xml" rel="self" />
  <link href="https://clirdlf.github.io/dlie_knowledge_graph/" />
  <updated>2025-06-09T00:00:00Z</updated>
  <id>https://clirdlf.github.io/dlie_knowledge_graph/</id>
  <author>
    <name>Council on Library and Information Resources</name>
  </author>
  <entry>
    <title>Contribute: Join the Digital Library of Integral Ecology</title>
    <link href="https://clirdlf.github.io/dlie_knowledge_graph/posts/contribute-join-the-digital-library-of-integral-ecology/" />
    <updated>2025-06-09T00:00:00Z</updated>
    <id>https://clirdlf.github.io/dlie_knowledge_graph/posts/contribute-join-the-digital-library-of-integral-ecology/</id>
    <content type="html">&lt;p&gt;This project is not just about building technology — it’s about building &lt;strong&gt;community&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;The Digital Library of Integral Ecology is a shared, open infrastructure for people who care about the Earth, its people, and our common future. It brings together knowledge from science, policy, spirituality, and grassroots voices — and we want you to be part of it.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;ways-to-get-involved&quot;&gt;Ways to Get Involved&lt;/h2&gt;&lt;p&gt;You don’t need to be a coder or a data scientist to contribute. Here are some ways anyone can help:&lt;/p&gt;&lt;h3 id=&quot;annotate-reports&quot;&gt;Annotate Reports&lt;/h3&gt;&lt;p&gt;Help tag ecological concepts, organizations, places, and ideas in documents using &lt;a href=&quot;https://github.com/doccano/doccano&quot;&gt;Doccano&lt;/a&gt;. No technical skill required — just your careful reading.&lt;/p&gt;&lt;h3 id=&quot;share-reports-and-sources&quot;&gt;Share Reports and Sources&lt;/h3&gt;&lt;p&gt;We’re always looking for ecological reports in &lt;strong&gt;different languages&lt;/strong&gt; and from &lt;strong&gt;diverse contexts&lt;/strong&gt; (NGOs, indigenous communities, policy, faith-based orgs, etc.).&lt;/p&gt;&lt;p&gt;Send us PDFs or links to reports that should be included in the graph.&lt;/p&gt;&lt;h3 id=&quot;train-the-ai&quot;&gt;Train the AI&lt;/h3&gt;&lt;p&gt;If you&#39;re technical, help us fine-tune our ecological NER models using annotated data. Or contribute to multilingual tagging tools and model evaluation.&lt;/p&gt;&lt;h3 id=&quot;translate-and-extend&quot;&gt;Translate and Extend&lt;/h3&gt;&lt;p&gt;Want to add new languages? New entity types like &lt;strong&gt;species&lt;/strong&gt;, &lt;strong&gt;spiritual practices&lt;/strong&gt;, or &lt;strong&gt;sacred sites&lt;/strong&gt;? We&#39;re building a framework that’s meant to grow with your contributions.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;technical-contributors&quot;&gt;Technical Contributors&lt;/h2&gt;&lt;p&gt;If you’re a developer, here’s where you can help:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Improve entity extraction and citation linking&lt;/li&gt;&lt;li&gt;Enhance multilingual NLP support&lt;/li&gt;&lt;li&gt;Automate ingestion from online archives&lt;/li&gt;&lt;li&gt;Build interactive search and visualization tools&lt;/li&gt;&lt;li&gt;Create public datasets from the graph (JSONL, CSV, RDF, etc.)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Our &lt;a href=&quot;https://github.com/clirdlf/dlie_knowledge_graph&quot;&gt;GitHub repository&lt;/a&gt; includes Docker-based workflows, NLP scripts, a Makefile, and blog documentation — all ready to clone and explore.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;how-to-get-started&quot;&gt;How to Get Started&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Star or fork the project on GitHub: &lt;a href=&quot;https://github.com/clirdlf/dlie_knowledge_graph&quot;&gt;github.com/clirdlf/dlie_knowledge_graph&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Clone the repo:&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;   &lt;span class=&quot;token function&quot;&gt;git&lt;/span&gt; clone https://github.com/clirdlf/dlie_knowledge_graph.git
   &lt;span class=&quot;token builtin class-name&quot;&gt;cd&lt;/span&gt; dlie_knowledge_graph
   &lt;span class=&quot;token function&quot;&gt;make&lt;/span&gt; up&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&quot;3&quot;&gt;&lt;li&gt;Try annotating or improving a model:&lt;/li&gt;&lt;/ol&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token function&quot;&gt;make&lt;/span&gt; pipeline &lt;span class=&quot;token assign-left variable&quot;&gt;PDF&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;my-report.pdf&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&quot;4&quot;&gt;&lt;li&gt;Join us on discussions (coming soon)&lt;/li&gt;&lt;/ol&gt;&lt;hr&gt;&lt;h2 id=&quot;we-re-building-this-together&quot;&gt;We’re Building This Together&lt;/h2&gt;&lt;p&gt;This project is inspired by the principles of &lt;strong&gt;integral ecology&lt;/strong&gt;, the idea that we must care for both the environment and the most vulnerable people it supports.&lt;/p&gt;&lt;p&gt;Building the Digital Library is a concrete act of hope. It’s a way to turn scattered, siloed knowledge into living, shared understanding.&lt;/p&gt;&lt;p&gt;Whether you’re a researcher, developer, librarian, artist, or student — you are welcome.&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt;&lt;p&gt;“All it takes is one good person to restore hope.” — Pope Francis, Laudato Si’&lt;/p&gt;&lt;/blockquote&gt;</content>
  </entry>
  <entry>
    <title>Evaluating the Results: Accuracy, Speed, and Insight</title>
    <link href="https://clirdlf.github.io/dlie_knowledge_graph/posts/evaluating-the-results-accuracy-speed-and-insight/" />
    <updated>2025-06-08T00:00:00Z</updated>
    <id>https://clirdlf.github.io/dlie_knowledge_graph/posts/evaluating-the-results-accuracy-speed-and-insight/</id>
    <content type="html">&lt;p&gt;We’ve come a long way — from PDFs to graphs, from automatic tagging to human annotation, and finally to training our own ecological NLP model.&lt;/p&gt;&lt;p&gt;But how do we know if it’s working?&lt;/p&gt;&lt;p&gt;In this post, we’ll explore how to evaluate your pipeline and model using both &lt;strong&gt;standard NLP metrics&lt;/strong&gt; and &lt;strong&gt;real-world ecological insight&lt;/strong&gt;.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;why-evaluate&quot;&gt;Why Evaluate?&lt;/h2&gt;&lt;p&gt;Evaluation helps us understand:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;✅ Is the model tagging entities accurately?&lt;/li&gt;&lt;li&gt;✅ Does it work well across languages?&lt;/li&gt;&lt;li&gt;✅ Is it fast enough for real-world use?&lt;/li&gt;&lt;li&gt;✅ What kinds of mistakes does it make?&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Evaluation isn’t just about numbers — it’s about improving &lt;strong&gt;trust&lt;/strong&gt;, &lt;strong&gt;transparency&lt;/strong&gt;, and &lt;strong&gt;impact&lt;/strong&gt;.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;key-metrics&quot;&gt;Key Metrics&lt;/h2&gt;&lt;h3 id=&quot;precision&quot;&gt;Precision&lt;/h3&gt;&lt;p&gt;The % of predicted entities that were correct&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Did it predict something meaningful?&lt;/p&gt;&lt;/blockquote&gt;&lt;h3 id=&quot;recall&quot;&gt;Recall&lt;/h3&gt;&lt;p&gt;The % of correct entities that were successfully predicted&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Did it miss important things?&lt;/p&gt;&lt;/blockquote&gt;&lt;h3 id=&quot;f1-score&quot;&gt;F1 Score&lt;/h3&gt;&lt;p&gt;The balance of precision and recall&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;A good all-around indicator&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;These metrics are calculated by comparing your model&#39;s output to &lt;strong&gt;human-annotated data&lt;/strong&gt;, usually exported from Doccano.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;how-to-evaluate-with-spacy&quot;&gt;How to Evaluate with spaCy&lt;/h2&gt;&lt;p&gt;After training, spaCy automatically evaluates on your dev/test set and shows:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;✔ Accuracy: &lt;span class=&quot;token number&quot;&gt;89.2&lt;/span&gt;%
✔ Precision: &lt;span class=&quot;token number&quot;&gt;88.5&lt;/span&gt;%
✔ Recall: &lt;span class=&quot;token number&quot;&gt;90.1&lt;/span&gt;%
✔ F1 Score: &lt;span class=&quot;token number&quot;&gt;89.3&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can also evaluate manually using:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;python &lt;span class=&quot;token parameter variable&quot;&gt;-m&lt;/span&gt; spacy evaluate ./output/model-best ./dev.spacy&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Or export annotated JSONL and compare predictions programmatically.&lt;/p&gt;&lt;h2 id=&quot;speed-and-runtime&quot;&gt;Speed and Runtime&lt;/h2&gt;&lt;p&gt;You can also evaluate &lt;strong&gt;how fast&lt;/strong&gt; your model runs:&lt;/p&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; spacy&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; time
nlp &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; spacy&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;output/model-best&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
text &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;The WWF is active in the Amazon on climate resilience.&quot;&lt;/span&gt;

start &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; time&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;time&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
doc &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; nlp&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;text&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ent&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;text&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; ent&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;label_&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; ent &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; doc&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;ents&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;Time:&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;time&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;time&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; start&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;seconds&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This helps you decide:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Which model is best for live pipelines vs. batch runs&lt;/li&gt;&lt;li&gt;When to use sm vs. &lt;code&gt;lg&lt;/code&gt; or &lt;code&gt;trf&lt;/code&gt; models&lt;/li&gt;&lt;/ul&gt;&lt;hr&gt;&lt;h2 id=&quot;beyond-accuracy-ecological-insight&quot;&gt;Beyond Accuracy: Ecological Insight&lt;/h2&gt;&lt;p&gt;Numbers matter, but so does &lt;strong&gt;usefulness&lt;/strong&gt;!&lt;/p&gt;&lt;p&gt;Ask:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Does the graph help us find new connections?&lt;/li&gt;&lt;li&gt;Are multilingual documents being fairly represented?&lt;/li&gt;&lt;li&gt;Does it highlight underrepresented organizations or regions?&lt;/li&gt;&lt;li&gt;Is it surfacing unexpected insights for researchers or communities?&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These qualitative measures can be just as valuable as F1 score.&lt;/p&gt;&lt;h2 id=&quot;try-it-yourself&quot;&gt;Try It Yourself&lt;/h2&gt;&lt;p&gt;After training, run:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;python &lt;span class=&quot;token parameter variable&quot;&gt;-m&lt;/span&gt; spacy evaluate output/model-best dev.spacy&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Or test it interactively:&lt;/p&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; spacy
nlp &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; spacy&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;output/model-best&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
doc &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; nlp&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;UNEP published a report on ecosystem resilience in East Africa.&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ent&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;text&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; ent&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;label_&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; ent &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; doc&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;ents&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then compare the results to the baseline model (&lt;code&gt;en_core_web_sm&lt;/code&gt;) or TaxoNERD.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;whats-next&quot;&gt;What&#39;s Next&lt;/h2&gt;&lt;p&gt;You’ve now seen the full lifecycle:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Extract text and citations from reports&lt;/li&gt;&lt;li&gt;Tag entities across languages&lt;/li&gt;&lt;li&gt;Upload to a knowledge graph&lt;/li&gt;&lt;li&gt;Annotate and refine human feedback&lt;/li&gt;&lt;li&gt;Train and evaluate your own model&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;You’re ready to contribute — or build your own ecological pipeline.&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>Training Our Own Ecological Language Model</title>
    <link href="https://clirdlf.github.io/dlie_knowledge_graph/posts/training-our-own-ecological-language-model/" />
    <updated>2025-06-07T00:00:00Z</updated>
    <id>https://clirdlf.github.io/dlie_knowledge_graph/posts/training-our-own-ecological-language-model/</id>
    <content type="html">&lt;h1 id=&quot;training-our-own-ecological-language-model&quot;&gt;Training Our Own Ecological Language Model&lt;/h1&gt;&lt;p&gt;We’ve built a working NLP pipeline that tags organizations, locations, and ecological concepts from multilingual reports. We’ve even used Doccano to correct and improve those tags.&lt;/p&gt;&lt;p&gt;Now it’s time to &lt;strong&gt;teach the system&lt;/strong&gt; to do better — by training our own custom NER model using the annotated data.&lt;/p&gt;&lt;p&gt;This post explains how we take human-labeled examples and turn them into a smarter, more accurate &lt;strong&gt;ecological language model&lt;/strong&gt;.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;why-train-a-custom-model&quot;&gt;Why Train a Custom Model?&lt;/h2&gt;&lt;p&gt;spaCy’s built-in models are trained on general-purpose data. They work well, but they don’t understand:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Domain-specific terms like &lt;em&gt;“climate resilience”&lt;/em&gt;, &lt;em&gt;“eco-spirituality”&lt;/em&gt;, or &lt;em&gt;“integral development”&lt;/em&gt;&lt;/li&gt;&lt;li&gt;New languages or spelling variants&lt;/li&gt;&lt;li&gt;Cross-domain relationships common in integral ecology (science + ethics + economics)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Training your own model lets you:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Add new entity types (e.g. &lt;code&gt;ECO_CONCEPT&lt;/code&gt;)&lt;/li&gt;&lt;li&gt;Improve accuracy for your documents&lt;/li&gt;&lt;li&gt;Adapt the system to your language and context&lt;/li&gt;&lt;/ul&gt;&lt;hr&gt;&lt;h2 id=&quot;tools-we-use&quot;&gt;Tools We Use&lt;/h2&gt;&lt;p&gt;We use &lt;a href=&quot;https://spacy.io&quot;&gt;spaCy&lt;/a&gt;’s training framework. It supports:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Training from scratch or fine-tuning&lt;/li&gt;&lt;li&gt;Evaluation metrics (precision, recall, F1)&lt;/li&gt;&lt;li&gt;Easy use of exported Doccano data&lt;/li&gt;&lt;/ul&gt;&lt;hr&gt;&lt;h2 id=&quot;from-doccano-to-training-data&quot;&gt;From Doccano to Training Data&lt;/h2&gt;&lt;p&gt;After annotating in Doccano:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Export your project as &lt;code&gt;.jsonl&lt;/code&gt;&lt;/li&gt;&lt;li&gt;Use a converter to turn that into spaCy format:&lt;/li&gt;&lt;/ol&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;spacy convert yourfile.jsonl ./train_data &lt;span class=&quot;token parameter variable&quot;&gt;--lang&lt;/span&gt; en &lt;span class=&quot;token parameter variable&quot;&gt;--ner&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This gives you &lt;code&gt;.spacy&lt;/code&gt; binary files for training.&lt;/p&gt;&lt;p&gt;Or use a Python script to convert to &lt;code&gt;train.json&lt;/code&gt;, &lt;code&gt;dev.json&lt;/code&gt; split manually.&lt;/p&gt;&lt;p&gt;⸻&lt;/p&gt;&lt;h2 id=&quot;training-the-model&quot;&gt;Training the Model&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;Create a config file:&lt;/li&gt;&lt;/ol&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;python &lt;span class=&quot;token parameter variable&quot;&gt;-m&lt;/span&gt; spacy init config ./config.cfg &lt;span class=&quot;token parameter variable&quot;&gt;--lang&lt;/span&gt; en &lt;span class=&quot;token parameter variable&quot;&gt;--pipeline&lt;/span&gt; ner&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Edit &lt;code&gt;config.cfg&lt;/code&gt;to match your needs (entity labels, GPU, batch size, etc.)&lt;/li&gt;&lt;li&gt;Prepare training assets:&lt;/li&gt;&lt;/ol&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;python &lt;span class=&quot;token parameter variable&quot;&gt;-m&lt;/span&gt; spacy init fill-config config.cfg config_final.cfg&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&quot;4&quot;&gt;&lt;li&gt;Train the model:&lt;/li&gt;&lt;/ol&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;python &lt;span class=&quot;token parameter variable&quot;&gt;-m&lt;/span&gt; spacy train config_final.cfg &lt;span class=&quot;token parameter variable&quot;&gt;--output&lt;/span&gt; ./output &lt;span class=&quot;token parameter variable&quot;&gt;--paths.train&lt;/span&gt; ./train.spacy &lt;span class=&quot;token parameter variable&quot;&gt;--paths.dev&lt;/span&gt; ./dev.spacy&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When done, your trained model will be in:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;./output/model-best&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can load it like this:&lt;/p&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; spacy
nlp &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; spacy&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;output/model-best&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;measuring-progress&quot;&gt;Measuring Progress&lt;/h2&gt;&lt;p&gt;During training, spaCy tracks:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Precision&lt;/strong&gt; — how many predicted entities were correct&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Recall&lt;/strong&gt; — how many true entities it found&lt;/li&gt;&lt;li&gt;&lt;strong&gt;F1 Score&lt;/strong&gt; — a balance of the two&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This helps you compare:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Pre-trained model (baseline)&lt;/li&gt;&lt;li&gt;Your fine-tuned model (specialized)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;⸻&lt;/p&gt;&lt;h2 id=&quot;try-it-yourself&quot;&gt;Try It Yourself&lt;/h2&gt;&lt;p&gt;Once you have annotations from Doccano:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token function&quot;&gt;make&lt;/span&gt; train-model&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Or step through each stage with:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;python &lt;span class=&quot;token parameter variable&quot;&gt;-m&lt;/span&gt; spacy init config ./config.cfg &lt;span class=&quot;token parameter variable&quot;&gt;--lang&lt;/span&gt; en &lt;span class=&quot;token parameter variable&quot;&gt;--pipeline&lt;/span&gt; ner
python &lt;span class=&quot;token parameter variable&quot;&gt;-m&lt;/span&gt; spacy train config.cfg &lt;span class=&quot;token parameter variable&quot;&gt;--output&lt;/span&gt; ./output &lt;span class=&quot;token parameter variable&quot;&gt;--paths.train&lt;/span&gt; ./train.spacy &lt;span class=&quot;token parameter variable&quot;&gt;--paths.dev&lt;/span&gt; ./dev.spacy&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Test the result:&lt;/p&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;nlp &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; spacy&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;output/model-best&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
doc &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; nlp&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;WWF works on climate resilience in the Amazon rainforest.&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;ent&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;text&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; ent&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;label_&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; ent &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; doc&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;ents&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&lt;h2 id=&quot;a-living-model&quot;&gt;A Living Model&lt;/h2&gt;&lt;p&gt;As more reports are added and more annotations made, we can:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Retrain the model periodically&lt;/li&gt;&lt;li&gt;Add multilingual support (e.g. &lt;code&gt;fr&lt;/code&gt;, &lt;code&gt;es&lt;/code&gt;, &lt;code&gt;zh&lt;/code&gt;)&lt;/li&gt;&lt;li&gt;Extend to more entity types (like &lt;code&gt;SPECIES&lt;/code&gt;, &lt;code&gt;DOCUMENT&lt;/code&gt;, &lt;code&gt;THEOLOGICAL_TERM&lt;/code&gt;)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This is how the Digital Library of Integral Ecology gets smarter over time — powered by human insight and collaborative learning.&lt;/p&gt;&lt;p&gt;⸻&lt;/p&gt;&lt;h2 id=&quot;what-s-next&quot;&gt;What’s Next?&lt;/h2&gt;&lt;p&gt;In the final post, we’ll evaluate the full system and compare how well the pipeline performs &lt;strong&gt;before and after training&lt;/strong&gt;, across languages and document types.&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>Annotation: Teaching the System to Be Smarter</title>
    <link href="https://clirdlf.github.io/dlie_knowledge_graph/posts/annotation-teaching-the-system-to-be-smarter/" />
    <updated>2025-06-06T00:00:00Z</updated>
    <id>https://clirdlf.github.io/dlie_knowledge_graph/posts/annotation-teaching-the-system-to-be-smarter/</id>
    <content type="html">&lt;p&gt;Our pipeline can already extract text from reports, tag entities, and build a multilingual knowledge graph — but how accurate is it?&lt;/p&gt;&lt;p&gt;The truth is: even good AI needs &lt;strong&gt;human correction&lt;/strong&gt;. That&#39;s where &lt;strong&gt;annotation&lt;/strong&gt; comes in.&lt;/p&gt;&lt;p&gt;In this post, we’ll show you how we use &lt;strong&gt;Doccano&lt;/strong&gt;, a friendly web interface, to correct the output of our pipeline and help the model improve over time.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;why-annotation-matters&quot;&gt;Why Annotation Matters&lt;/h2&gt;&lt;p&gt;Even the best models make mistakes:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Misclassifying entities (e.g. &amp;quot;Amazon&amp;quot; as a product instead of a forest)&lt;/li&gt;&lt;li&gt;Missing subtle ecological terms (like “resilience” or “eco-conversion”)&lt;/li&gt;&lt;li&gt;Struggling with languages like Arabic or complex phrases&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Annotation lets humans:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Correct the labels&lt;/li&gt;&lt;li&gt;Add missing terms&lt;/li&gt;&lt;li&gt;Build reliable training data&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;It’s like proofreading — but for a machine learning system.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;what-is-doccano&quot;&gt;What is Doccano?&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://doccano.github.io/doccano/&quot;&gt;Doccano&lt;/a&gt; is an open-source tool for &lt;strong&gt;annotating text for NLP tasks&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;It provides:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A browser-based interface&lt;/li&gt;&lt;li&gt;Support for named entity recognition (NER), classification, translation, and more&lt;/li&gt;&lt;li&gt;Role-based user access&lt;/li&gt;&lt;li&gt;Easy data import and export&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We use Doccano to refine the results of &lt;code&gt;ner_pipeline.py&lt;/code&gt;.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;exporting-to-doccano-format&quot;&gt;Exporting to Doccano Format&lt;/h2&gt;&lt;p&gt;After entity tagging, we export the results in &lt;a href=&quot;https://github.com/doccano/doccano/blob/master/docs/api.md#annotation-format&quot;&gt;JSONL format&lt;/a&gt;:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;python export_doccano.py my-report.entities.json&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Which creates:&lt;/p&gt;&lt;pre class=&quot;language-json&quot;&gt;&lt;code class=&quot;language-json&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;The WWF report on the Amazon rainforest...&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;&quot;labels&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;ORG&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;36&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;LOC&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This format can be imported directly into Doccano.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;using-doccano-locally&quot;&gt;Using Doccano Locally&lt;/h2&gt;&lt;p&gt;If you’re running the project with Docker Compose, Doccano is already available at:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://localhost:8000&quot;&gt;http://localhost:8000&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Log in with&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Username:&lt;/strong&gt; &lt;code&gt;admin&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Password:&lt;/strong&gt; &lt;code&gt;password&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;From there, you can&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Create a new NER project&lt;/li&gt;&lt;li&gt;Import your &lt;code&gt;.jsonl&lt;/code&gt; file (generated in previous step)&lt;/li&gt;&lt;li&gt;Start tagging!&lt;/li&gt;&lt;/ol&gt;&lt;hr&gt;&lt;h2 id=&quot;the-feedback-loop&quot;&gt;The Feedback Loop&lt;/h2&gt;&lt;p&gt;Once documents are annotated in Doccano, we:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Export the clean annotations&lt;/li&gt;&lt;li&gt;Convert them to spaCy training format&lt;/li&gt;&lt;li&gt;Fine-tune a custom NER model&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;This cycle helps the system:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Improve tagging accuracy&lt;/li&gt;&lt;li&gt;Recognize new or uncommon terms&lt;/li&gt;&lt;li&gt;Adapt to multilingual and ecological contexts&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We call this process &lt;strong&gt;active learning&lt;/strong&gt;.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;try-it-yourself&quot;&gt;Try It Yourself&lt;/h2&gt;&lt;p&gt;If you’ve already run:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token function&quot;&gt;make&lt;/span&gt; pipeline &lt;span class=&quot;token assign-left variable&quot;&gt;PDF&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;my-report.pdf&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then a Doccano file will be created at:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;data/doccano/my-report.entities.jsonl&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Import this file into your Doccano project and try refining the labels!&lt;/p&gt;&lt;h2 id=&quot;who-can-help&quot;&gt;Who Can Help?&lt;/h2&gt;&lt;p&gt;Annotation is one of the best ways to contribute, especially if you:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Are a researcher in ecology, theology, or social sciences&lt;/li&gt;&lt;li&gt;Are bilingual or multilingual&lt;/li&gt;&lt;li&gt;Want to help shape AI to better understand the world&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All you need is careful attention. &lt;strong&gt;No coding required!&lt;/strong&gt;&lt;/p&gt;&lt;h2 id=&quot;what-s-next&quot;&gt;What’s Next?&lt;/h2&gt;&lt;p&gt;Now that we have clean, annotated data, we’re ready to train our own &lt;strong&gt;ecologically informed NER model&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;In the next post, we’ll walk through how to train a custom spaCy pipeline using your Doccano data.&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>Stitching the Graph: Saving Knowledge to Neo4j</title>
    <link href="https://clirdlf.github.io/dlie_knowledge_graph/posts/stitching-the-graph-saving-knowledge-to-neo4j/" />
    <updated>2025-06-05T00:00:00Z</updated>
    <id>https://clirdlf.github.io/dlie_knowledge_graph/posts/stitching-the-graph-saving-knowledge-to-neo4j/</id>
    <content type="html">&lt;p&gt;So far, we’ve extracted clean text from ecological reports and used AI to identify important entities like organizations, places, and ecological concepts.&lt;/p&gt;&lt;p&gt;But identifying entities is only half the story. Now we need to &lt;strong&gt;connect&lt;/strong&gt; them — to build our &lt;strong&gt;knowledge graph&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;In this post, we show how we use &lt;strong&gt;Neo4j&lt;/strong&gt;, a graph database, to stitch everything together into a network of knowledge.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;what-is-a-graph-database&quot;&gt;What is a Graph Database?&lt;/h2&gt;&lt;p&gt;A &lt;strong&gt;graph database&lt;/strong&gt; stores information as &lt;strong&gt;nodes&lt;/strong&gt; (things) and &lt;strong&gt;relationships&lt;/strong&gt; (connections).&lt;/p&gt;&lt;p&gt;Unlike traditional databases, which store rows and columns, a graph lets you explore:&lt;/p&gt;&lt;pre class=&quot;language-plaintext&quot;&gt;&lt;code class=&quot;language-plaintext&quot;&gt;(WWF Report) –MENTIONS–&gt; (Amazon rainforest)
–CITES—–&gt; (IPBES 2019 Report)
–MENTIONS–&gt; (climate resilience)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Neo4j is the most popular open-source graph database. It’s:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Visual&lt;/li&gt;&lt;li&gt;Easy to query (using a language called Cypher)&lt;/li&gt;&lt;li&gt;Designed to handle relationships efficiently&lt;/li&gt;&lt;/ul&gt;&lt;hr&gt;&lt;h2 id=&quot;what-we-store-in-the-graph&quot;&gt;🧱 What We Store in the Graph&lt;/h2&gt;&lt;p&gt;Each document and entity we extract becomes a &lt;strong&gt;node&lt;/strong&gt;, and their connections are modeled as &lt;strong&gt;relationships&lt;/strong&gt;.&lt;/p&gt;&lt;h3 id=&quot;node-types&quot;&gt;Node Types&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;:Document&lt;/code&gt; — the report itself&lt;/li&gt;&lt;li&gt;&lt;code&gt;:Entity&lt;/code&gt; — an extracted item (e.g. “WWF”, “Amazon rainforest”)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Each node has properties like:&lt;/p&gt;&lt;pre class=&quot;language-cypher&quot;&gt;&lt;code class=&quot;language-cypher&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token class-name&quot;&gt;Document&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;name&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;wwf_amazon_report&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; lang&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token class-name&quot;&gt;Entity&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;text&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;climate resilience&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; label&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;ECO_CONCEPT&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;relationship-types&quot;&gt;Relationship Types&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;(:Document)-[:MENTIONS]-&amp;gt;(:Entity)&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;(:Document)-[:CITES]-&amp;gt;(:Document)&lt;/code&gt; (coming soon via citation parsing)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These relationships make the knowledge &lt;strong&gt;navigable&lt;/strong&gt;, not just searchable.&lt;/p&gt;&lt;h2 id=&quot;how-it-works-in-code&quot;&gt;How It Works in Code&lt;/h2&gt;&lt;p&gt;After entity tagging, we load the data into Neo4j using:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;python graph_upload.py my-report.entities.json&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This script:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Loads the entity JSON file&lt;/li&gt;&lt;li&gt;Creates the &lt;code&gt;:Document&lt;/code&gt; node&lt;/li&gt;&lt;li&gt;Creates one &lt;code&gt;:Entity&lt;/code&gt; node per unique mention&lt;/li&gt;&lt;li&gt;Connects them with &lt;code&gt;:MENTIONS&lt;/code&gt; relationships&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;You can view this graph in Neo4j’s browser:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://localhost:7474&quot;&gt;http://localhost:7474&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Use the default login:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Username:&lt;/strong&gt; neo4j&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Password:&lt;/strong&gt; password&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;And run a query like:&lt;/p&gt;&lt;pre class=&quot;language-cypher&quot;&gt;&lt;code class=&quot;language-cypher&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;MATCH&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;d&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token class-name&quot;&gt;Document&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token relationship property&quot;&gt;MENTIONS&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&gt;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;e&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token class-name&quot;&gt;Entity&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;RETURN&lt;/span&gt; d&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; e&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&lt;h2 id=&quot;try-it-yourself&quot;&gt;Try It Yourself&lt;/h2&gt;&lt;p&gt;If you’ve run the full pipeline:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token function&quot;&gt;make&lt;/span&gt; pipeline &lt;span class=&quot;token assign-left variable&quot;&gt;PDF&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;my-report.pdf&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The upload to Neo4j is done automatically. Otherwise, you can run it directly:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token function&quot;&gt;docker&lt;/span&gt; compose &lt;span class=&quot;token builtin class-name&quot;&gt;exec&lt;/span&gt; worker python graph_upload.py my-report.entities.json&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then open Neo4j in your browser and start exploring the graph!&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;what-can-you-do-with-it&quot;&gt;What Can You Do With It?&lt;/h2&gt;&lt;p&gt;Once in the graph, you can:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Find all documents mentioning a specific concept&lt;/li&gt;&lt;li&gt;Discover what organizations work in similar regions&lt;/li&gt;&lt;li&gt;Visualize themes and citations across languages&lt;/li&gt;&lt;li&gt;Prepare data for annotation or deeper AI training&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This is where our static data becomes &lt;strong&gt;living knowledge&lt;/strong&gt;.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;what-s-next&quot;&gt;What’s Next?&lt;/h2&gt;&lt;p&gt;Now that our knowledge is structured and searchable, we’ll look at how to &lt;strong&gt;export the data to Doccano&lt;/strong&gt;, a simple annotation tool that lets humans teach the system to improve over time.&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>Tagging the World: Finding Places, Plants, and Ideas with AI</title>
    <link href="https://clirdlf.github.io/dlie_knowledge_graph/posts/tagging-the-world-finding-places-plants-and-ideas-with-ai/" />
    <updated>2025-06-04T00:00:00Z</updated>
    <id>https://clirdlf.github.io/dlie_knowledge_graph/posts/tagging-the-world-finding-places-plants-and-ideas-with-ai/</id>
    <content type="html">&lt;p&gt;Once we&#39;ve extracted clean text from a PDF, the next step is to &lt;strong&gt;understand what’s being talked about&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;That’s where Natural Language Processing (NLP) comes in.&lt;/p&gt;&lt;p&gt;We use NLP to scan the text and find key pieces of information — like:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Locations (e.g. &amp;quot;Amazon rainforest&amp;quot;)&lt;/li&gt;&lt;li&gt;Organizations (e.g. &amp;quot;WWF&amp;quot;)&lt;/li&gt;&lt;li&gt;Ecological concepts (e.g. &amp;quot;resilience&amp;quot;, &amp;quot;biodiversity loss&amp;quot;)&lt;/li&gt;&lt;li&gt;Citations (e.g. &amp;quot;IPBES 2019 Report&amp;quot;)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Each of these is called an &lt;strong&gt;entity&lt;/strong&gt;, and this process is called &lt;strong&gt;Named Entity Recognition (NER)&lt;/strong&gt;.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;what-is-named-entity-recognition&quot;&gt;What is Named Entity Recognition?&lt;/h2&gt;&lt;p&gt;NER is a type of AI model that reads text and labels the parts that represent real-world things.&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;pre class=&quot;language-plaintext&quot;&gt;&lt;code class=&quot;language-plaintext&quot;&gt;Original text:
The WWF report on the Amazon rainforest highlights climate resilience strategies.

NER output:
[ORG: WWF], [LOC: Amazon rainforest], [ECO_CONCEPT: climate resilience]&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This gives us structured data from unstructured sentences, and helps us populate our knowledge graph with &lt;strong&gt;nodes and connections&lt;/strong&gt;.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;tools-we-use&quot;&gt;Tools We Use&lt;/h2&gt;&lt;h3 id=&quot;spacy&quot;&gt;spaCy&lt;/h3&gt;&lt;p&gt;We use &lt;a href=&quot;https://spacy.io&quot;&gt;spaCy&lt;/a&gt;, a popular open-source NLP library that can:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Work in English, Spanish, French, Chinese, Russian, and more&lt;/li&gt;&lt;li&gt;Recognize standard entities like ORG, LOC, PERSON, etc.&lt;/li&gt;&lt;li&gt;Run fast and integrate easily with Python&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;taxonerd&quot;&gt;TaxoNERD&lt;/h3&gt;&lt;p&gt;For ecological texts, general NLP isn’t enough — so we also use &lt;a href=&quot;https://github.com/nleguillarme/taxonerd&quot;&gt;TaxoNERD&lt;/a&gt;, a tool trained to detect ecological and taxonomic entities, like:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Ecosystem types&lt;/li&gt;&lt;li&gt;Species groups&lt;/li&gt;&lt;li&gt;Environmental terms&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/nleguillarme/taxonerd&quot;&gt;TaxoNERD&lt;/a&gt; uses a model called &lt;a href=&quot;https://github.com/dmis-lab/biobert&quot;&gt;BioBERT&lt;/a&gt; and is specialized for ecological language.&lt;/p&gt;&lt;h3 id=&quot;multilingual-support&quot;&gt;Multilingual Support&lt;/h3&gt;&lt;p&gt;We also include spaCy models for:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://spacy.io/models/fr#fr_core_news_lg&quot;&gt;fr_core_news_lg&lt;/a&gt; (French)&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://spacy.io/models/es#es_core_news_lg&quot;&gt;es_core_news_lg&lt;/a&gt; (Spanish)&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://spacy.io/models/zh#zh_core_web_trf&quot;&gt;zh_core_web_trf&lt;/a&gt; (Chinese)&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://spacy.io/models/xx#xx_ent_wiki_sm&quot;&gt;xx_ent_wiki_sm&lt;/a&gt; (basic multilingual)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This makes the system &lt;strong&gt;language-aware&lt;/strong&gt;, even when documents span continents.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;how-it-works-in-code&quot;&gt;How It Works in Code&lt;/h2&gt;&lt;p&gt;The tagging is handled by this command:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;python ner_pipeline.py my-report.txt&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Which produces:&lt;/p&gt;&lt;pre class=&quot;language-json&quot;&gt;&lt;code class=&quot;language-json&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;...&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token property&quot;&gt;&quot;start&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token property&quot;&gt;&quot;end&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;22&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token property&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;LOC&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token property&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;Amazon rainforest&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token property&quot;&gt;&quot;start&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;31&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token property&quot;&gt;&quot;end&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;34&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token property&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;ORG&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token property&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;WWF&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token property&quot;&gt;&quot;start&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;45&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token property&quot;&gt;&quot;end&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;63&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token property&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;ECO_CONCEPT&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token property&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;climate resilience&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is saved as &lt;code&gt;my-report.entities.json&lt;/code&gt; in the &lt;code&gt;data/output/&lt;/code&gt; directory.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;why-this-matters&quot;&gt;Why This Matters&lt;/h2&gt;&lt;p&gt;Recognizing entities allows us to:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Link a sentence to the right concepts&lt;/li&gt;&lt;li&gt;Group reports by theme or region&lt;/li&gt;&lt;li&gt;Connect related documents, even across languages&lt;/li&gt;&lt;li&gt;Support annotation and model training&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;It’s the first step toward turning plain text into a semantic map.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;try-it-yourself&quot;&gt;Try It Yourself&lt;/h2&gt;&lt;p&gt;If you’ve already extracted text using:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token function&quot;&gt;make&lt;/span&gt; pipeline &lt;span class=&quot;token assign-left variable&quot;&gt;PDF&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;/data/input/sample1.pdf&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The entity tagging will run automatically. Check the output in:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;data/output/sample1.entities.json&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can also run it independently:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token function&quot;&gt;docker&lt;/span&gt; compose &lt;span class=&quot;token builtin class-name&quot;&gt;exec&lt;/span&gt; worker python ner_pipeline.py /data/input/sample1.txt&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&lt;h2 id=&quot;what-s-next&quot;&gt;What’s Next?&lt;/h2&gt;&lt;p&gt;Next, we’ll take these entities and load them into Neo4j — our graph database — where we can start to visualize and query relationships.&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>From PDF to Text: Extracting Meaning from Documents</title>
    <link href="https://clirdlf.github.io/dlie_knowledge_graph/posts/from-pdf-to-text-extracting-meaning-from-documents/" />
    <updated>2025-06-03T00:00:00Z</updated>
    <id>https://clirdlf.github.io/dlie_knowledge_graph/posts/from-pdf-to-text-extracting-meaning-from-documents/</id>
    <content type="html">&lt;p&gt;The first step in building a knowledge graph for integral ecology is simple in concept — but tricky in practice:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;How do we get clean, usable &lt;strong&gt;text&lt;/strong&gt; from messy, multilingual &lt;strong&gt;PDF reports&lt;/strong&gt;?&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;This post explains how we extract both the &lt;strong&gt;raw text&lt;/strong&gt; and the &lt;strong&gt;structured citations&lt;/strong&gt; from reports using two tools:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://pymupdf.readthedocs.io/en/latest/index.html&quot;&gt;PyMuPDF&lt;/a&gt; for text&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://grobid.readthedocs.io/en/latest/&quot;&gt;GROBID&lt;/a&gt; for citations and metadata&lt;/li&gt;&lt;/ul&gt;&lt;hr&gt;&lt;h2 id=&quot;why-this-matters&quot;&gt;Why This Matters&lt;/h2&gt;&lt;p&gt;PDFs are designed for printing, not for reading by machines.&lt;/p&gt;&lt;p&gt;They can include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Columns and footnotes&lt;/li&gt;&lt;li&gt;Images, tables, and scanned pages&lt;/li&gt;&lt;li&gt;Embedded fonts or malformed characters&lt;/li&gt;&lt;li&gt;Multiple languages in one document&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If we want to detect entities and link knowledge later, we need high-quality &lt;strong&gt;plain text&lt;/strong&gt;.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;step-1-extract-text-with-pymupdf&quot;&gt;Step 1: Extract Text with PyMuPDF&lt;/h2&gt;&lt;p&gt;We use &lt;a href=&quot;https://pymupdf.readthedocs.io/&quot;&gt;&lt;code&gt;PyMuPDF&lt;/code&gt;&lt;/a&gt; (also known as &lt;code&gt;fitz&lt;/code&gt;) to extract the text page-by-page from a PDF.&lt;/p&gt;&lt;p&gt;It:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Preserves layout well&lt;/li&gt;&lt;li&gt;Handles multiple languages&lt;/li&gt;&lt;li&gt;Works with scanned+OCR’d documents if text is embedded&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Example output:&lt;/strong&gt;&lt;/p&gt;&lt;pre class=&quot;language-plaintext&quot;&gt;&lt;code class=&quot;language-plaintext&quot;&gt;The Amazon rainforest is shrinking rapidly.

WWF reported that deforestation increased 12% in 2023.&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This text gets saved as &lt;code&gt;report.txt&lt;/code&gt;.&lt;/p&gt;&lt;h2 id=&quot;step-2-extract-citations-with-grobid&quot;&gt;Step 2: Extract Citations with GROBID&lt;/h2&gt;&lt;p&gt;Next, we use &lt;a href=&quot;https://github.com/kermitt2/grobid&quot;&gt;GROBID&lt;/a&gt;, a tool that reads the bibliography and metadata of academic papers and reports.&lt;/p&gt;&lt;p&gt;GROBID converts messy citation lists like:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;[12] Smith, J., “Biodiversity and Forests”, Nature, 2020&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Into structured, machine-readable &lt;strong&gt;TEI XML&lt;/strong&gt;, which can include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Title&lt;/li&gt;&lt;li&gt;Authors&lt;/li&gt;&lt;li&gt;Year&lt;/li&gt;&lt;li&gt;Journal or publisher&lt;/li&gt;&lt;li&gt;DOI or identifiers&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We save this as &lt;code&gt;report.biblio.xml&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Later in the pipeline, this will help us:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Build &lt;strong&gt;:CITES relationships&lt;/strong&gt; in the graph&lt;/li&gt;&lt;li&gt;Match references across reports&lt;/li&gt;&lt;li&gt;Cluster similar reports by their sources&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;how-this-works-in-code&quot;&gt;How This Works in Code&lt;/h2&gt;&lt;p&gt;Our system includes a script that does this automatically:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;python extract_text.py my-report.pdf&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It will:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Extract the full text using PyMuPDF → &lt;code&gt;my-report.txt&lt;/code&gt;&lt;/li&gt;&lt;li&gt;Send the PDF to the GROBID API → &lt;code&gt;my-report.biblio.xml&lt;/code&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The script runs inside a Docker container and outputs files to the &lt;code&gt;/data/output/&lt;/code&gt; folder.&lt;/p&gt;&lt;h2 id=&quot;try-it-yourself&quot;&gt;Try It Yourself&lt;/h2&gt;&lt;p&gt;If you’ve followed the setup from &lt;a href=&quot;https://clirdlf.github.io/dlie_knowledge_graph/posts/20_building_blocks/&quot;&gt;Part 2&lt;/a&gt;, you can run:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token function&quot;&gt;make&lt;/span&gt; pipeline &lt;span class=&quot;token assign-left variable&quot;&gt;PDF&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;my-report.pdf&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Extract text and citations&lt;/li&gt;&lt;li&gt;Tag entities (coming up in Part 4)&lt;/li&gt;&lt;li&gt;Load data into Neo4j&lt;/li&gt;&lt;li&gt;Export annotated text for review&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Check the results in &lt;code&gt;data/output/&lt;/code&gt; — you’ll see &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.entities.json&lt;/code&gt;, and &lt;code&gt;.biblio.xml&lt;/code&gt; files.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;what-we-learned&quot;&gt;What We Learned&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;PDFs are tricky, but PyMuPDF gives us reliable plain text&lt;/li&gt;&lt;li&gt;GROBID gives us structured citations, ready for linking&lt;/li&gt;&lt;li&gt;Clean text is the foundation for everything that follows&lt;/li&gt;&lt;/ul&gt;&lt;hr&gt;&lt;h2 id=&quot;what-s-next&quot;&gt;What’s Next?&lt;/h2&gt;&lt;p&gt;In the next post, we’ll explore how we tag entities in the text using AI — recognizing organizations, locations, ecological concepts, and more.&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>Building Blocks: Documents, Entities, and Relationships</title>
    <link href="https://clirdlf.github.io/dlie_knowledge_graph/posts/building-blocks-documents-entities-and-relationships/" />
    <updated>2025-06-02T00:00:00Z</updated>
    <id>https://clirdlf.github.io/dlie_knowledge_graph/posts/building-blocks-documents-entities-and-relationships/</id>
    <content type="html">&lt;p&gt;In our last post, we introduced the vision: building a knowledge graph for &lt;strong&gt;integral ecology&lt;/strong&gt;, a way to connect people, places, organizations, and ideas across languages and disciplines.&lt;/p&gt;&lt;p&gt;But how exactly do we turn messy PDFs and complex reports into a meaningful, searchable web of knowledge?&lt;/p&gt;&lt;p&gt;We start with three key building blocks:&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;1-documents&quot;&gt;1. Documents&lt;/h2&gt;&lt;p&gt;The foundation of our knowledge graph is a &lt;strong&gt;document&lt;/strong&gt; — a report, academic paper, policy brief, or even a faith-based reflection.&lt;/p&gt;&lt;p&gt;Each document is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A single file (usually a PDF)&lt;/li&gt;&lt;li&gt;With a title, language, source (UNEP, WWF, OpenAlex, etc.)&lt;/li&gt;&lt;li&gt;That contains many sentences — and lots of &lt;strong&gt;information hidden in plain text&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We treat each document as a &lt;strong&gt;node&lt;/strong&gt; in the graph, and from there, we extract meaning.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;2-entities&quot;&gt;2. Entities&lt;/h2&gt;&lt;p&gt;Entities are &lt;strong&gt;things that the document talks about&lt;/strong&gt; — people, organizations, species, places, and ecological concepts.&lt;/p&gt;&lt;p&gt;For example:&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Entity Text&lt;/th&gt;&lt;th&gt;Label&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Amazon rainforest&lt;/td&gt;&lt;td&gt;&lt;code&gt;LOCATION&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;WWF&lt;/td&gt;&lt;td&gt;&lt;code&gt;ORG&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;climate resilience&lt;/td&gt;&lt;td&gt;&lt;code&gt;ECO_CONCEPT&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Laudato Si’&lt;/td&gt;&lt;td&gt;&lt;code&gt;DOCUMENT&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Our system uses &lt;strong&gt;Natural Language Processing (NLP)&lt;/strong&gt; tools to automatically recognize these entities in many languages — with models trained on large text collections.&lt;/p&gt;&lt;p&gt;Later, we’ll even &lt;strong&gt;fine-tune our own models&lt;/strong&gt; to be more accurate for ecological language.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;3-relationships&quot;&gt;3. Relationships&lt;/h2&gt;&lt;p&gt;The real power of a knowledge graph comes from the &lt;strong&gt;connections between entities&lt;/strong&gt; — also called &lt;strong&gt;edges&lt;/strong&gt; or &lt;strong&gt;relationships&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;Some examples:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A document &lt;strong&gt;MENTIONS&lt;/strong&gt; an entity&lt;/li&gt;&lt;li&gt;A document &lt;strong&gt;CITES&lt;/strong&gt; another document&lt;/li&gt;&lt;li&gt;A concept &lt;strong&gt;IS_RELATED_TO&lt;/strong&gt; another concept&lt;/li&gt;&lt;li&gt;An organization &lt;strong&gt;WORKS_IN&lt;/strong&gt; a specific region&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These relationships turn isolated data points into an &lt;strong&gt;interconnected network&lt;/strong&gt; — where you can explore patterns, paths, and shared meaning.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;putting-it-together&quot;&gt;Putting It Together&lt;/h2&gt;&lt;p&gt;Here&#39;s a simple example:&lt;/p&gt;&lt;pre class=&quot;language-cypher&quot;&gt;&lt;code class=&quot;language-cypher&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;WWF Report&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;–MENTIONS–&lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;Amazon rainforest&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
–MENTIONS–&lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;climate resilience&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
–CITES—–&lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;IPBES &lt;span class=&quot;token number&quot;&gt;2022&lt;/span&gt; Report&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the graph database, each of these is a &lt;strong&gt;node&lt;/strong&gt; (document or entity) and each arrow is a &lt;strong&gt;relationship&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;We can now:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Search for all reports mentioning &amp;quot;climate resilience&amp;quot;&lt;/li&gt;&lt;li&gt;Find which NGOs cite a particular scientific assessment&lt;/li&gt;&lt;li&gt;Map ecological priorities across language and region&lt;/li&gt;&lt;/ul&gt;&lt;hr&gt;&lt;h2 id=&quot;why-it-matters&quot;&gt;Why It Matters&lt;/h2&gt;&lt;p&gt;This model gives us:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Structure&lt;/strong&gt; — so we can search and analyze consistently&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt; — works for hundreds or thousands of documents&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Interoperability&lt;/strong&gt; — can be visualized, queried, and shared&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;And it sets the stage for automation, collaboration, and learning.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;try-it-yourself-clone-and-run-the-project&quot;&gt;Try It Yourself: Clone &amp;amp; Run the Project&lt;/h2&gt;&lt;p&gt;You can explore and run this pipeline locally using Docker.&lt;/p&gt;&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; (Desktop or CLI)&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://git-scm.com/&quot;&gt;Git&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;step-1-clone-the-repository&quot;&gt;Step 1: Clone the Repository&lt;/h3&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token function&quot;&gt;git&lt;/span&gt; clone https://github.com/clirdlf/dlie_knowledge_graph.git
&lt;span class=&quot;token builtin class-name&quot;&gt;cd&lt;/span&gt; dlie_knowledge_graph&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;step-2-build-and-start-the-system&quot;&gt;Step 2: Build and Start the System&lt;/h3&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token function&quot;&gt;make&lt;/span&gt; build
&lt;span class=&quot;token function&quot;&gt;make&lt;/span&gt; up&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will spin up:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://grobid.readthedocs.io/en/latest/&quot;&gt;GROBID&lt;/a&gt; for citation parsing&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://neo4j.com/&quot;&gt;Neo4j&lt;/a&gt; for the knowledge graph&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://doccano.github.io/doccano/&quot;&gt;Doccano&lt;/a&gt; for annotation&lt;/li&gt;&lt;li&gt;A Python environment for text and entity extraction&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Then, you can run the full pipeline like this:&lt;/p&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token function&quot;&gt;make&lt;/span&gt; pipeline &lt;span class=&quot;token assign-left variable&quot;&gt;PDF&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;/data/input/202206_IPBES_GLOBALREPORT.pdf&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You’ll find the results in the &lt;code&gt;data/output/&lt;/code&gt; and &lt;code&gt;data/doccano/&lt;/code&gt; folders.&lt;/p&gt;&lt;h2 id=&quot;what-s-next&quot;&gt;What’s Next?&lt;/h2&gt;&lt;p&gt;In the next post, we’ll start &lt;strong&gt;extracting text from real reports&lt;/strong&gt; — even messy PDFs — using smart tools like PyMuPDF and GROBID.&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>What is a Knowledge Graph for Integral Ecology?</title>
    <link href="https://clirdlf.github.io/dlie_knowledge_graph/posts/what-is-a-knowledge-graph-for-integral-ecology/" />
    <updated>2025-06-01T00:00:00Z</updated>
    <id>https://clirdlf.github.io/dlie_knowledge_graph/posts/what-is-a-knowledge-graph-for-integral-ecology/</id>
    <content type="html">&lt;p&gt;We live in an age of overwhelming information, but ecological knowledge, wisdom, and action often remain &lt;strong&gt;fragmented&lt;/strong&gt;, buried in reports, scattered across languages, or locked in formats only specialists can access.&lt;/p&gt;&lt;p&gt;That’s where the &lt;strong&gt;Digital Library of Integral Ecology&lt;/strong&gt; comes in. Our mission is to bring this information together, from scientific papers to NGO reports to faith-based reflections, into a single, interconnected digital library that helps researchers, educators, and communities act for the common good. One of the tools that helps us discovery and interrogate integral ecology is a knowledge graph. This codebase and set of tutorials will walk you through the technologies, opportunities, and tradeoffs in building this feature of the Digitial Library of Integral Ecology.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;what-do-we-mean-by-integral-ecology&quot;&gt;What Do We Mean by &amp;quot;Integral Ecology&amp;quot;?&lt;/h2&gt;&lt;p&gt;The term &lt;em&gt;integral ecology&lt;/em&gt; comes from &lt;em&gt;&lt;a href=&quot;https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html&quot;&gt;Laudato Si’&lt;/a&gt;&lt;/em&gt;, Pope Francis&#39; encyclical on the environment. It’s about recognizing the &lt;strong&gt;deep connections between ecological, social, cultural, and spiritual concerns&lt;/strong&gt;. Climate change, deforestation, loss of biodiversity — these aren’t just technical problems. They are moral ones, economic ones, and spiritual ones too.&lt;/p&gt;&lt;p&gt;Integral ecology asks us to &lt;strong&gt;think in systems&lt;/strong&gt;, and to see how everything is connected.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;and-what-s-a-knowledge-graph&quot;&gt;And What’s a Knowledge Graph?&lt;/h2&gt;&lt;p&gt;A &lt;strong&gt;knowledge graph&lt;/strong&gt; is a way of storing and exploring knowledge by looking at &lt;strong&gt;relationships&lt;/strong&gt;. Imagine a big web:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A report mentions &lt;strong&gt;“Amazon rainforest”&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;It connects to a &lt;strong&gt;location&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;The report is published by &lt;strong&gt;UNESCO&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;It discusses concepts like &lt;strong&gt;resilience&lt;/strong&gt; and &lt;strong&gt;biodiversity&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;It cites other documents that are connected too&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All of this is represented not just as flat text, but as &lt;strong&gt;linked data&lt;/strong&gt; — relationships between concepts, people, places, and ideas. This is what lets us ask better questions and see deeper patterns.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;what-we-re-building&quot;&gt;What We&#39;re Building&lt;/h2&gt;&lt;p&gt;We are creating a system that:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Extracts text&lt;/strong&gt; from ecological reports and scientific papers (even in PDF format)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Identifies key concepts&lt;/strong&gt;, organizations, places, species, and ideas — in multiple languages&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Links them together&lt;/strong&gt; in a searchable, visual graph (using Neo4j)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Lets people annotate&lt;/strong&gt; and refine that knowledge with simple tools&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Trains smarter AI models&lt;/strong&gt; over time that understand ecology more deeply&lt;/li&gt;&lt;/ol&gt;&lt;hr&gt;&lt;h2 id=&quot;why-it-matters&quot;&gt;Why It Matters&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Researchers can discover connections across disciplines and languages&lt;/li&gt;&lt;li&gt;NGOs can map their work to broader systems and goals&lt;/li&gt;&lt;li&gt;Educators can explore real-world ecological examples interactively&lt;/li&gt;&lt;li&gt;Communities can build shared understanding of their bioregions&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We believe this is a project not just of technology — but of &lt;strong&gt;ecological conversion&lt;/strong&gt;.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;what-s-next&quot;&gt;What’s Next?&lt;/h2&gt;&lt;p&gt;In the next post, we’ll walk through the building blocks of the graph: &lt;strong&gt;documents, entities, and relationships&lt;/strong&gt;, and how we transform text into structure.&lt;/p&gt;</content>
  </entry>
</feed>