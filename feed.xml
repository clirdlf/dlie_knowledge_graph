<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="/dlie_knowledge_graph/feed.xml" rel="self" type="application/atom+xml" /><link href="/dlie_knowledge_graph/" rel="alternate" type="text/html" /><updated>2025-07-08T19:48:49+00:00</updated><id>/dlie_knowledge_graph/feed.xml</id><title type="html">Digital Library of Integral Ecology</title><subtitle>A tutorial series on building a knowledge graph for ecology</subtitle><entry><title type="html">Contribute: Join the Digital Library of Integral Ecology</title><link href="/dlie_knowledge_graph/2025/07/04/How-you-can-help-build-and-grow-this-open-knowledge-commons" rel="alternate" type="text/html" title="Contribute: Join the Digital Library of Integral Ecology" /><published>2025-07-04T00:00:00+00:00</published><updated>2025-07-04T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/07/04/How%20you%20can%20help%20build%20and%20grow%20this%20open%20knowledge%20commons.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/07/04/How-you-can-help-build-and-grow-this-open-knowledge-commons"><![CDATA[<p>This project is not just about building technology ‚Äî it‚Äôs about building <strong>community</strong>.</p>

<p>The Digital Library of Integral Ecology is a shared, open infrastructure for people who care about the Earth, its people, and our common future. It brings together knowledge from science, policy, spirituality, and grassroots voices ‚Äî and we want you to be part of it.</p>

<hr />

<h2 id="ways-to-get-involved">Ways to Get Involved</h2>

<p>You don‚Äôt need to be a coder or a data scientist to contribute. Here are some ways anyone can help:</p>

<h3 id="annotate-reports">Annotate Reports</h3>
<p>Help tag ecological concepts, organizations, places, and ideas in documents using <a href="https://github.com/doccano/doccano">Doccano</a>. No technical skill required ‚Äî just your careful reading.</p>

<h3 id="share-reports-and-sources">Share Reports and Sources</h3>
<p>We‚Äôre always looking for ecological reports in <strong>different languages</strong> and from <strong>diverse contexts</strong> (NGOs, indigenous communities, policy, faith-based orgs, etc.).</p>

<p>Send us PDFs or links to reports that should be included in the graph.</p>

<h3 id="train-the-ai">Train the AI</h3>
<p>If you‚Äôre technical, help us fine-tune our ecological NER models using annotated data. Or contribute to multilingual tagging tools and model evaluation.</p>

<h3 id="translate-and-extend">Translate and Extend</h3>
<p>Want to add new languages? New entity types like <strong>species</strong>, <strong>spiritual practices</strong>, or <strong>sacred sites</strong>? We‚Äôre building a framework that‚Äôs meant to grow with your contributions.</p>

<hr />

<h2 id="technical-contributors">Technical Contributors</h2>

<p>If you‚Äôre a developer, here‚Äôs where you can help:</p>

<ul>
  <li>Improve entity extraction and citation linking</li>
  <li>Enhance multilingual NLP support</li>
  <li>Automate ingestion from online archives</li>
  <li>Build interactive search and visualization tools</li>
  <li>Create public datasets from the graph (JSONL, CSV, RDF, etc.)</li>
</ul>

<p>Our <a href="https://github.com/clirdlf/dlie_knowledge_graph">GitHub repository</a> includes Docker-based workflows, NLP scripts, a Makefile, and blog documentation ‚Äî all ready to clone and explore.</p>

<hr />

<h2 id="how-to-get-started">How to Get Started</h2>

<ol>
  <li>
    <p>Star or fork the project on GitHub: <a href="https://github.com/clirdlf/dlie_knowledge_graph">github.com/clirdlf/dlie_knowledge_graph</a></p>
  </li>
  <li>
    <p>Clone the repo:</p>
  </li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   git clone https://github.com/clirdlf/dlie_knowledge_graph.git
   <span class="nb">cd </span>dlie_knowledge_graph
   make up
</code></pre></div></div>

<ol>
  <li>Try annotating or improving a model:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make pipeline <span class="nv">PDF</span><span class="o">=</span>my-report.pdf
</code></pre></div></div>

<ol>
  <li>Join us on discussions (coming soon)</li>
</ol>

<hr />

<h2 id="were-building-this-together">We‚Äôre Building This Together</h2>

<p>This project is inspired by the principles of <strong>integral ecology</strong>,  the idea that we must care for both the environment and the most vulnerable people it supports.</p>

<p>Building the Digital Library is a concrete act of hope. It‚Äôs a way to turn scattered, siloed knowledge into living, shared understanding.</p>

<p>Whether you‚Äôre a researcher, developer, librarian, artist, or student ‚Äî you are welcome.</p>

<hr />

<blockquote>
  <p>‚ÄúAll it takes is one good person to restore hope.‚Äù ‚Äî Pope Francis, Laudato Si‚Äô</p>
</blockquote>]]></content><author><name></name></author><summary type="html"><![CDATA[This project is not just about building technology ‚Äî it‚Äôs about building community.]]></summary></entry><entry><title type="html">Training Our Own Ecological Language Model</title><link href="/dlie_knowledge_graph/2025/07/03/Turning-annotations-into-a-smarter,-ecology-specific-NER-system" rel="alternate" type="text/html" title="Training Our Own Ecological Language Model" /><published>2025-07-03T00:00:00+00:00</published><updated>2025-07-03T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/07/03/Turning%20annotations%20into%20a%20smarter,%20ecology-specific%20NER%20system.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/07/03/Turning-annotations-into-a-smarter,-ecology-specific-NER-system"><![CDATA[<h1 id="training-our-own-ecological-language-model">Training Our Own Ecological Language Model</h1>

<p>We‚Äôve built a working NLP pipeline that tags organizations, locations, and ecological concepts from multilingual reports. We‚Äôve even used Doccano to correct and improve those tags.</p>

<p>Now it‚Äôs time to <strong>teach the system</strong> to do better ‚Äî by training our own custom NER model using the annotated data.</p>

<p>This post explains how we take human-labeled examples and turn them into a smarter, more accurate <strong>ecological language model</strong>.</p>

<hr />

<h2 id="why-train-a-custom-model">Why Train a Custom Model?</h2>

<p>spaCy‚Äôs built-in models are trained on general-purpose data. They work well, but they don‚Äôt understand:</p>

<ul>
  <li>Domain-specific terms like <em>‚Äúclimate resilience‚Äù</em>, <em>‚Äúeco-spirituality‚Äù</em>, or <em>‚Äúintegral development‚Äù</em></li>
  <li>New languages or spelling variants</li>
  <li>Cross-domain relationships common in integral ecology (science + ethics + economics)</li>
</ul>

<p>Training your own model lets you:</p>

<ul>
  <li>Add new entity types (e.g. <code class="language-plaintext highlighter-rouge">ECO_CONCEPT</code>)</li>
  <li>Improve accuracy for your documents</li>
  <li>Adapt the system to your language and context</li>
</ul>

<hr />

<h2 id="tools-we-use">Tools We Use</h2>

<p>We use <a href="https://spacy.io">spaCy</a>‚Äôs training framework. It supports:</p>

<ul>
  <li>Training from scratch or fine-tuning</li>
  <li>Evaluation metrics (precision, recall, F1)</li>
  <li>Easy use of exported Doccano data</li>
</ul>

<hr />

<h2 id="from-doccano-to-training-data">From Doccano to Training Data</h2>

<p>After annotating in Doccano:</p>

<ol>
  <li>Export your project as <code class="language-plaintext highlighter-rouge">.jsonl</code></li>
  <li>Use a converter to turn that into spaCy format:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spacy convert yourfile.jsonl ./train_data <span class="nt">--lang</span> en <span class="nt">--ner</span>
</code></pre></div></div>

<p>This gives you <code class="language-plaintext highlighter-rouge">.spacy</code> binary files for training.</p>

<p>Or use a Python script to convert to <code class="language-plaintext highlighter-rouge">train.json</code>, <code class="language-plaintext highlighter-rouge">dev.json</code> split manually.</p>

<p>‚∏ª</p>

<h2 id="training-the-model">Training the Model</h2>

<ol>
  <li>Create a config file:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> spacy init config ./config.cfg <span class="nt">--lang</span> en <span class="nt">--pipeline</span> ner
</code></pre></div></div>

<ol>
  <li>Edit <code class="language-plaintext highlighter-rouge">config.cfg </code>to match your needs (entity labels, GPU, batch size, etc.)</li>
  <li>Prepare training assets:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> spacy init fill-config config.cfg config_final.cfg
</code></pre></div></div>

<ol>
  <li>Train the model:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> spacy train config_final.cfg <span class="nt">--output</span> ./output <span class="nt">--paths</span>.train ./train.spacy <span class="nt">--paths</span>.dev ./dev.spacy
</code></pre></div></div>

<p>When done, your trained model will be in:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./output/model-best
</code></pre></div></div>

<p>You can load it like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">output/model-best</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="measuring-progress">Measuring Progress</h2>

<p>During training, spaCy tracks:</p>

<ul>
  <li><strong>Precision</strong> ‚Äî how many predicted entities were correct</li>
  <li><strong>Recall</strong> ‚Äî how many true entities it found</li>
  <li><strong>F1 Score</strong> ‚Äî a balance of the two</li>
</ul>

<p>This helps you compare:</p>

<ul>
  <li>Pre-trained model (baseline)</li>
  <li>Your fine-tuned model (specialized)</li>
</ul>

<p>‚∏ª</p>

<h2 id="try-it-yourself">Try It Yourself</h2>

<p>Once you have annotations from Doccano:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make train-model
</code></pre></div></div>

<p>Or step through each stage with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> spacy init config ./config.cfg <span class="nt">--lang</span> en <span class="nt">--pipeline</span> ner
python <span class="nt">-m</span> spacy train config.cfg <span class="nt">--output</span> ./output <span class="nt">--paths</span>.train ./train.spacy <span class="nt">--paths</span>.dev ./dev.spacy
</code></pre></div></div>

<p>Test the result:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">output/model-best</span><span class="sh">"</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="nf">nlp</span><span class="p">(</span><span class="sh">"</span><span class="s">WWF works on climate resilience in the Amazon rainforest.</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">([(</span><span class="n">ent</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="n">ent</span><span class="p">.</span><span class="n">label_</span><span class="p">)</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">.</span><span class="n">ents</span><span class="p">])</span>
</code></pre></div></div>

<hr />

<h2 id="a-living-model">A Living Model</h2>

<p>As more reports are added and more annotations made, we can:</p>

<ul>
  <li>Retrain the model periodically</li>
  <li>Add multilingual support (e.g. <code class="language-plaintext highlighter-rouge">fr</code>, <code class="language-plaintext highlighter-rouge">es</code>, <code class="language-plaintext highlighter-rouge">zh</code>)</li>
  <li>Extend to more entity types (like <code class="language-plaintext highlighter-rouge">SPECIES</code>, <code class="language-plaintext highlighter-rouge">DOCUMENT</code>, <code class="language-plaintext highlighter-rouge">THEOLOGICAL_TERM</code>)</li>
</ul>

<p>This is how the Digital Library of Integral Ecology gets smarter over time ‚Äî powered by human insight and collaborative learning.</p>

<p>‚∏ª</p>

<h2 id="whats-next">What‚Äôs Next?</h2>

<p>In the final post, we‚Äôll evaluate the full system and compare how well the pipeline performs <strong>before and after training</strong>, across languages and document types.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Training Our Own Ecological Language Model]]></summary></entry><entry><title type="html">Stitching the Graph: Saving Knowledge to Neo4j</title><link href="/dlie_knowledge_graph/2025/07/02/How-our-documents-and-entities-are-stored-and-visualized-as-a-graph" rel="alternate" type="text/html" title="Stitching the Graph: Saving Knowledge to Neo4j" /><published>2025-07-02T00:00:00+00:00</published><updated>2025-07-02T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/07/02/How%20our%20documents%20and%20entities%20are%20stored%20and%20visualized%20as%20a%20graph.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/07/02/How-our-documents-and-entities-are-stored-and-visualized-as-a-graph"><![CDATA[<p>So far, we‚Äôve extracted clean text from ecological reports and used AI to identify important entities like organizations, places, and ecological concepts.</p>

<p>But identifying entities is only half the story. Now we need to <strong>connect</strong> them ‚Äî to build our <strong>knowledge graph</strong>.</p>

<p>In this post, we show how we use <strong>Neo4j</strong>, a graph database, to stitch everything together into a network of knowledge.</p>

<hr />

<h2 id="what-is-a-graph-database">What is a Graph Database?</h2>

<p>A <strong>graph database</strong> stores information as <strong>nodes</strong> (things) and <strong>relationships</strong> (connections).</p>

<p>Unlike traditional databases, which store rows and columns, a graph lets you explore:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(WWF Report) ‚ÄìMENTIONS‚Äì&gt; (Amazon rainforest)
‚ÄìCITES‚Äî‚Äì&gt; (IPBES 2019 Report)
‚ÄìMENTIONS‚Äì&gt; (climate resilience)
</code></pre></div></div>

<p>Neo4j is the most popular open-source graph database. It‚Äôs:</p>
<ul>
  <li>Visual</li>
  <li>Easy to query (using a language called Cypher)</li>
  <li>Designed to handle relationships efficiently</li>
</ul>

<hr />

<h2 id="-what-we-store-in-the-graph">üß± What We Store in the Graph</h2>

<p>Each document and entity we extract becomes a <strong>node</strong>, and their connections are modeled as <strong>relationships</strong>.</p>

<h3 id="node-types">Node Types</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">:Document</code> ‚Äî the report itself</li>
  <li><code class="language-plaintext highlighter-rouge">:Entity</code> ‚Äî an extracted item (e.g. ‚ÄúWWF‚Äù, ‚ÄúAmazon rainforest‚Äù)</li>
</ul>

<p>Each node has properties like:</p>

<div class="language-cypher highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ss">(</span><span class="nc">:Document</span> <span class="ss">{</span><span class="py">name:</span> <span class="s2">"wwf_amazon_report"</span><span class="ss">,</span> <span class="py">lang:</span> <span class="s2">"en"</span><span class="ss">})</span>
<span class="ss">(</span><span class="nc">:Entity</span> <span class="ss">{</span><span class="py">text:</span> <span class="s2">"climate resilience"</span><span class="ss">,</span> <span class="py">label:</span> <span class="s2">"ECO_CONCEPT"</span><span class="ss">})</span>
</code></pre></div></div>

<h3 id="relationship-types">Relationship Types</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">(:Document)-[:MENTIONS]-&gt;(:Entity)</code></li>
  <li><code class="language-plaintext highlighter-rouge">(:Document)-[:CITES]-&gt;(:Document)</code> (coming soon via citation parsing)</li>
</ul>

<p>These relationships make the knowledge <strong>navigable</strong>, not just searchable.</p>

<h2 id="how-it-works-in-code">How It Works in Code</h2>

<p>After entity tagging, we load the data into Neo4j using:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python graph_upload.py my-report.entities.json
</code></pre></div></div>

<p>This script:</p>

<ol>
  <li>Loads the entity JSON file</li>
  <li>Creates the <code class="language-plaintext highlighter-rouge">:Document</code> node</li>
  <li>Creates one <code class="language-plaintext highlighter-rouge">:Entity</code> node per unique mention</li>
  <li>Connects them with <code class="language-plaintext highlighter-rouge">:MENTIONS</code> relationships</li>
</ol>

<p>You can view this graph in Neo4j‚Äôs browser:</p>

<p><a href="http://localhost:7474">http://localhost:7474</a></p>

<p>Use the default login:</p>

<ul>
  <li><strong>Username:</strong> neo4j</li>
  <li><strong>Password:</strong> password</li>
</ul>

<p>And run a query like:</p>

<div class="language-cypher highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">MATCH</span><span class="w"> </span><span class="ss">(</span><span class="py">d:</span><span class="n">Document</span><span class="ss">)</span><span class="o">-</span><span class="ss">[</span><span class="nc">:MENTIONS</span><span class="ss">]</span><span class="o">-&gt;</span><span class="ss">(</span><span class="py">e:</span><span class="n">Entity</span><span class="ss">)</span>
<span class="k">RETURN</span> <span class="n">d</span><span class="ss">,</span> <span class="n">e</span>
</code></pre></div></div>

<hr />

<h2 id="try-it-yourself">Try It Yourself</h2>

<p>If you‚Äôve run the full pipeline:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make pipeline <span class="nv">PDF</span><span class="o">=</span>my-report.pdf
</code></pre></div></div>

<p>The upload to Neo4j is done automatically. Otherwise, you can run it directly:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose <span class="nb">exec </span>worker python graph_upload.py my-report.entities.json
</code></pre></div></div>

<p>Then open Neo4j in your browser and start exploring the graph!</p>

<hr />

<h2 id="what-can-you-do-with-it">What Can You Do With It?</h2>

<p>Once in the graph, you can:</p>

<ul>
  <li>Find all documents mentioning a specific concept</li>
  <li>Discover what organizations work in similar regions</li>
  <li>Visualize themes and citations across languages</li>
  <li>Prepare data for annotation or deeper AI training</li>
</ul>

<p>This is where our static data becomes <strong>living knowledge</strong>.</p>

<hr />

<h2 id="whats-next">What‚Äôs Next?</h2>

<p>Now that our knowledge is structured and searchable, we‚Äôll look at how to <strong>export the data to Doccano</strong>, a simple annotation tool that lets humans teach the system to improve over time.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[So far, we‚Äôve extracted clean text from ecological reports and used AI to identify important entities like organizations, places, and ecological concepts.]]></summary></entry><entry><title type="html">Evaluating the Results: Accuracy, Speed, and Insight</title><link href="/dlie_knowledge_graph/2025/07/01/Measuring-how-well-our-models-work,-and-what-they-help-us-uncover" rel="alternate" type="text/html" title="Evaluating the Results: Accuracy, Speed, and Insight" /><published>2025-07-01T00:00:00+00:00</published><updated>2025-07-01T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/07/01/Measuring%20how%20well%20our%20models%20work,%20and%20what%20they%20help%20us%20uncover.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/07/01/Measuring-how-well-our-models-work,-and-what-they-help-us-uncover"><![CDATA[<p>We‚Äôve come a long way ‚Äî from PDFs to graphs, from automatic tagging to human annotation, and finally to training our own ecological NLP model.</p>

<p>But how do we know if it‚Äôs working?</p>

<p>In this post, we‚Äôll explore how to evaluate your pipeline and model using both <strong>standard NLP metrics</strong> and <strong>real-world ecological insight</strong>.</p>

<hr />

<h2 id="why-evaluate">Why Evaluate?</h2>

<p>Evaluation helps us understand:</p>

<ul>
  <li>‚úÖ Is the model tagging entities accurately?</li>
  <li>‚úÖ Does it work well across languages?</li>
  <li>‚úÖ Is it fast enough for real-world use?</li>
  <li>‚úÖ What kinds of mistakes does it make?</li>
</ul>

<p>Evaluation isn‚Äôt just about numbers ‚Äî it‚Äôs about improving <strong>trust</strong>, <strong>transparency</strong>, and <strong>impact</strong>.</p>

<hr />

<h2 id="key-metrics">Key Metrics</h2>

<h3 id="precision">Precision</h3>
<p>The % of predicted entities that were correct</p>
<blockquote>
  <p>Did it predict something meaningful?</p>
</blockquote>

<h3 id="recall">Recall</h3>
<p>The % of correct entities that were successfully predicted</p>
<blockquote>
  <p>Did it miss important things?</p>
</blockquote>

<h3 id="f1-score">F1 Score</h3>
<p>The balance of precision and recall</p>
<blockquote>
  <p>A good all-around indicator</p>
</blockquote>

<p>These metrics are calculated by comparing your model‚Äôs output to <strong>human-annotated data</strong>, usually exported from Doccano.</p>

<hr />

<h2 id="how-to-evaluate-with-spacy">How to Evaluate with spaCy</h2>

<p>After training, spaCy automatically evaluates on your dev/test set and shows:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>‚úî Accuracy: 89.2%
‚úî Precision: 88.5%
‚úî Recall: 90.1%
‚úî F1 Score: 89.3
</code></pre></div></div>

<p>You can also evaluate manually using:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> spacy evaluate ./output/model-best ./dev.spacy
</code></pre></div></div>

<p>Or export annotated JSONL and compare predictions programmatically.</p>

<h2 id="speed-and-runtime">Speed and Runtime</h2>

<p>You can also evaluate <strong>how fast</strong> your model runs:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">spacy</span><span class="p">,</span> <span class="n">time</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">output/model-best</span><span class="sh">"</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">The WWF is active in the Amazon on climate resilience.</span><span class="sh">"</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">doc</span> <span class="o">=</span> <span class="nf">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nf">print</span><span class="p">([(</span><span class="n">ent</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="n">ent</span><span class="p">.</span><span class="n">label_</span><span class="p">)</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">.</span><span class="n">ents</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Time:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="sh">"</span><span class="s">seconds</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>This helps you decide:</p>

<ul>
  <li>Which model is best for live pipelines vs. batch runs</li>
  <li>When to use sm vs. <code class="language-plaintext highlighter-rouge">lg</code> or <code class="language-plaintext highlighter-rouge">trf</code> models</li>
</ul>

<hr />

<h2 id="beyond-accuracy-ecological-insight">Beyond Accuracy: Ecological Insight</h2>

<p>Numbers matter, but so does <strong>usefulness</strong>!</p>

<p>Ask:</p>

<ul>
  <li>Does the graph help us find new connections?</li>
  <li>Are multilingual documents being fairly represented?</li>
  <li>Does it highlight underrepresented organizations or regions?</li>
  <li>Is it surfacing unexpected insights for researchers or communities?</li>
</ul>

<p>These qualitative measures can be just as valuable as F1 score.</p>

<h2 id="try-it-yourself">Try It Yourself</h2>

<p>After training, run:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> spacy evaluate output/model-best dev.spacy
</code></pre></div></div>

<p>Or test it interactively:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">output/model-best</span><span class="sh">"</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="nf">nlp</span><span class="p">(</span><span class="sh">"</span><span class="s">UNEP published a report on ecosystem resilience in East Africa.</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">([(</span><span class="n">ent</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="n">ent</span><span class="p">.</span><span class="n">label_</span><span class="p">)</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">.</span><span class="n">ents</span><span class="p">])</span>
</code></pre></div></div>

<p>Then compare the results to the baseline model (<code class="language-plaintext highlighter-rouge">en_core_web_sm</code>) or TaxoNERD.</p>

<hr />

<h2 id="whats-next">What‚Äôs Next</h2>

<p>You‚Äôve now seen the full lifecycle:</p>

<ol>
  <li>Extract text and citations from reports</li>
  <li>Tag entities across languages</li>
  <li>Upload to a knowledge graph</li>
  <li>Annotate and refine human feedback</li>
  <li>Train and evaluate your own model</li>
</ol>

<p>You‚Äôre ready to contribute ‚Äî or build your own ecological pipeline.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[We‚Äôve come a long way ‚Äî from PDFs to graphs, from automatic tagging to human annotation, and finally to training our own ecological NLP model.]]></summary></entry><entry><title type="html">Annotation: Teaching the System to Be Smarter</title><link href="/dlie_knowledge_graph/2025/06/29/A-look-at-Doccano-and-how-humans-refine-machine-learning" rel="alternate" type="text/html" title="Annotation: Teaching the System to Be Smarter" /><published>2025-06-29T00:00:00+00:00</published><updated>2025-06-29T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/06/29/A%20look%20at%20Doccano%20and%20how%20humans%20refine%20machine%20learning.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/06/29/A-look-at-Doccano-and-how-humans-refine-machine-learning"><![CDATA[<p>Our pipeline can already extract text from reports, tag entities, and build a multilingual knowledge graph ‚Äî but how accurate is it?</p>

<p>The truth is: even good AI needs <strong>human correction</strong>. That‚Äôs where <strong>annotation</strong> comes in.</p>

<p>In this post, we‚Äôll show you how we use <strong>Doccano</strong>, a friendly web interface, to correct the output of our pipeline and help the model improve over time.</p>

<hr />

<h2 id="why-annotation-matters">Why Annotation Matters</h2>

<p>Even the best models make mistakes:</p>

<ul>
  <li>Misclassifying entities (e.g. ‚ÄúAmazon‚Äù as a product instead of a forest)</li>
  <li>Missing subtle ecological terms (like ‚Äúresilience‚Äù or ‚Äúeco-conversion‚Äù)</li>
  <li>Struggling with languages like Arabic or complex phrases</li>
</ul>

<p>Annotation lets humans:</p>

<ul>
  <li>Correct the labels</li>
  <li>Add missing terms</li>
  <li>Build reliable training data</li>
</ul>

<p>It‚Äôs like proofreading ‚Äî but for a machine learning system.</p>

<hr />

<h2 id="what-is-doccano">What is Doccano?</h2>

<p><a href="https://doccano.github.io/doccano/">Doccano</a> is an open-source tool for <strong>annotating text for NLP tasks</strong>.</p>

<p>It provides:</p>

<ul>
  <li>A browser-based interface</li>
  <li>Support for named entity recognition (NER), classification, translation, and more</li>
  <li>Role-based user access</li>
  <li>Easy data import and export</li>
</ul>

<p>We use Doccano to refine the results of <code class="language-plaintext highlighter-rouge">ner_pipeline.py</code>.</p>

<hr />

<h2 id="exporting-to-doccano-format">Exporting to Doccano Format</h2>

<p>After entity tagging, we export the results in <a href="https://github.com/doccano/doccano/blob/master/docs/api.md#annotation-format">JSONL format</a>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python export_doccano.py my-report.entities.json
</code></pre></div></div>

<p>Which creates:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"The WWF report on the Amazon rainforest..."</span><span class="p">,</span><span class="w">
  </span><span class="nl">"labels"</span><span class="p">:</span><span class="w"> </span><span class="p">[[</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="s2">"ORG"</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="mi">20</span><span class="p">,</span><span class="w"> </span><span class="mi">36</span><span class="p">,</span><span class="w"> </span><span class="s2">"LOC"</span><span class="p">]]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>This format can be imported directly into Doccano.</p>

<hr />

<h2 id="using-doccano-locally">Using Doccano Locally</h2>

<p>If you‚Äôre running the project with Docker Compose, Doccano is already available at:</p>

<p><a href="http://localhost:8000">http://localhost:8000</a></p>

<p>Log in with</p>

<ul>
  <li><strong>Username:</strong> admin</li>
  <li><strong>Password:</strong> admin</li>
</ul>

<p>From there, you can</p>

<ol>
  <li>Create a new NER project</li>
  <li>Import your .jsonl file (generated in previous step)</li>
  <li>Start tagging!</li>
</ol>

<hr />

<h2 id="the-feedback-loop">The Feedback Loop</h2>

<p>Once documents are annotated in Doccano, we:</p>

<ol>
  <li>Export the clean annotations</li>
  <li>Convert them to spaCy training format</li>
  <li>Fine-tune a custom NER model</li>
</ol>

<p>This cycle helps the system:</p>

<ul>
  <li>Improve tagging accuracy</li>
  <li>Recognize new or uncommon terms</li>
  <li>Adapt to multilingual and ecological contexts</li>
</ul>

<p>We call this process <strong>active learning</strong>.</p>

<hr />

<h2 id="try-it-yourself">Try It Yourself</h2>

<p>If you‚Äôve already run:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make pipeline <span class="nv">PDF</span><span class="o">=</span>my-report.pdf
</code></pre></div></div>

<p>Then a Doccano file will be created at:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data/doccano/my-report.entities.jsonl
</code></pre></div></div>

<p>Import this file into your Doccano project and try refining the labels!</p>

<h2 id="who-can-help">Who Can Help?</h2>

<p>Annotation is one of the best ways to contribute, especially if you:</p>

<ul>
  <li>Are a researcher in ecology, theology, or social sciences</li>
  <li>Are bilingual or multilingual</li>
  <li>Want to help shape AI to better understand the world</li>
</ul>

<p>All you need is careful attention. <strong>No coding required!</strong></p>

<h2 id="whats-next">What‚Äôs Next?</h2>

<p>Now that we have clean, annotated data, we‚Äôre ready to train our own <strong>ecologically informed NER model</strong>.</p>

<p>In the next post, we‚Äôll walk through how to train a custom spaCy pipeline using your Doccano data.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Our pipeline can already extract text from reports, tag entities, and build a multilingual knowledge graph ‚Äî but how accurate is it?]]></summary></entry><entry><title type="html">Tagging the World: Finding Places, Plants, and Ideas with AI</title><link href="/dlie_knowledge_graph/2025/06/28/How-NLP-tools-identify-ecological-concepts,-organizations,-and-more" rel="alternate" type="text/html" title="Tagging the World: Finding Places, Plants, and Ideas with AI" /><published>2025-06-28T00:00:00+00:00</published><updated>2025-06-28T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/06/28/How%20NLP%20tools%20identify%20ecological%20concepts,%20organizations,%20and%20more.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/06/28/How-NLP-tools-identify-ecological-concepts,-organizations,-and-more"><![CDATA[<p>Once we‚Äôve extracted clean text from a PDF, the next step is to <strong>understand what‚Äôs being talked about</strong>.</p>

<p>That‚Äôs where Natural Language Processing (NLP) comes in.</p>

<p>We use NLP to scan the text and find key pieces of information ‚Äî like:</p>

<ul>
  <li>Locations (e.g. ‚ÄúAmazon rainforest‚Äù)</li>
  <li>Organizations (e.g. ‚ÄúWWF‚Äù)</li>
  <li>Ecological concepts (e.g. ‚Äúresilience‚Äù, ‚Äúbiodiversity loss‚Äù)</li>
  <li>Citations (e.g. ‚ÄúIPBES 2019 Report‚Äù)</li>
</ul>

<p>Each of these is called an <strong>entity</strong>, and this process is called <strong>Named Entity Recognition (NER)</strong>.</p>

<hr />

<h2 id="what-is-named-entity-recognition">What is Named Entity Recognition?</h2>

<p>NER is a type of AI model that reads text and labels the parts that represent real-world things.</p>

<p>Example:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original text:
The WWF report on the Amazon rainforest highlights climate resilience strategies.

NER output:
[ORG: WWF], [LOC: Amazon rainforest], [ECO_CONCEPT: climate resilience]
</code></pre></div></div>

<p>This gives us structured data from unstructured sentences, and helps us populate our knowledge graph with <strong>nodes and connections</strong>.</p>

<hr />

<h2 id="tools-we-use">Tools We Use</h2>

<h3 id="spacy">spaCy</h3>

<p>We use spaCy, a popular open-source NLP library that can:</p>

<ul>
  <li>Work in English, Spanish, French, Chinese, Russian, and more</li>
  <li>Recognize standard entities like ORG, LOC, PERSON, etc.</li>
  <li>Run fast and integrate easily with Python</li>
</ul>

<h3 id="taxonerd">TaxoNERD</h3>

<p>For ecological texts, general NLP isn‚Äôt enough ‚Äî so we also use TaxoNERD, a tool trained to detect ecological and taxonomic entities, like:</p>

<ul>
  <li>Ecosystem types</li>
  <li>Species groups</li>
  <li>Environmental terms</li>
</ul>

<p>TaxoNERD uses a model called BioBERT and is specialized for ecological language.</p>

<h3 id="multilingual-support">Multilingual Support</h3>

<p>We also include spaCy models for:</p>

<ul>
  <li>fr_core_news_lg (French)</li>
  <li>es_core_news_lg (Spanish)</li>
  <li>zh_core_web_trf (Chinese)</li>
  <li>xx_ent_wiki_sm (basic multilingual)</li>
</ul>

<p>This makes the system l<strong>anguage-aware</strong>, even when documents span continents.</p>

<hr />

<h2 id="how-it-works-in-code">How It Works in Code</h2>

<p>The tagging is handled by this command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python ner_pipeline.py my-report.txt
</code></pre></div></div>

<p>Which produces:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"..."</span><span class="p">,</span><span class="w">
  </span><span class="nl">"entities"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="nl">"start"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="nl">"end"</span><span class="p">:</span><span class="w"> </span><span class="mi">22</span><span class="p">,</span><span class="w"> </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"LOC"</span><span class="p">,</span><span class="w"> </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Amazon rainforest"</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"start"</span><span class="p">:</span><span class="w"> </span><span class="mi">31</span><span class="p">,</span><span class="w"> </span><span class="nl">"end"</span><span class="p">:</span><span class="w"> </span><span class="mi">34</span><span class="p">,</span><span class="w"> </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ORG"</span><span class="p">,</span><span class="w"> </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"WWF"</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"start"</span><span class="p">:</span><span class="w"> </span><span class="mi">45</span><span class="p">,</span><span class="w"> </span><span class="nl">"end"</span><span class="p">:</span><span class="w"> </span><span class="mi">63</span><span class="p">,</span><span class="w"> </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ECO_CONCEPT"</span><span class="p">,</span><span class="w"> </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"climate resilience"</span><span class="p">}</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>This is saved as <code class="language-plaintext highlighter-rouge">my-report.entities.json</code> in the <code class="language-plaintext highlighter-rouge">data/output/</code> folder.</p>

<hr />

<h2 id="why-this-matters">Why This Matters</h2>

<p>Recognizing entities allows us to:</p>
<ul>
  <li>Link a sentence to the right concepts</li>
  <li>Group reports by theme or region</li>
  <li>Connect related documents, even across languages</li>
  <li>Support annotation and model training</li>
</ul>

<p>It‚Äôs the first step toward turning plain text into a semantic map.</p>

<hr />

<h2 id="try-it-yourself">Try It Yourself</h2>

<p>If you‚Äôve already extracted text using:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make pipeline <span class="nv">PDF</span><span class="o">=</span>my-report.pdf
</code></pre></div></div>

<p>The entity tagging will run automatically. Check the output in:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data/output/my-report.entities.json
</code></pre></div></div>

<p>You can also run it independently:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose <span class="nb">exec </span>worker python ner_pipeline.py my-report.txt
</code></pre></div></div>

<hr />

<h2 id="whats-next">What‚Äôs Next?</h2>

<p>Next, we‚Äôll take these entities and load them into Neo4j ‚Äî our graph database ‚Äî where we can start to visualize and query relationships.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Once we‚Äôve extracted clean text from a PDF, the next step is to understand what‚Äôs being talked about.]]></summary></entry><entry><title type="html">From PDF to Text: Extracting Meaning from Documents</title><link href="/dlie_knowledge_graph/2025/06/27/Learn-how-we-pull-clean,-useful-text-from-messy-PDFs-and-scientific-citations" rel="alternate" type="text/html" title="From PDF to Text: Extracting Meaning from Documents" /><published>2025-06-27T00:00:00+00:00</published><updated>2025-06-27T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/06/27/Learn%20how%20we%20pull%20clean,%20useful%20text%20from%20messy%20PDFs%20and%20scientific%20citations.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/06/27/Learn-how-we-pull-clean,-useful-text-from-messy-PDFs-and-scientific-citations"><![CDATA[<p>The first step in building a knowledge graph for integral ecology is simple in concept ‚Äî but tricky in practice:</p>

<blockquote>
  <p>How do we get clean, usable <strong>text</strong> from messy, multilingual <strong>PDF reports</strong>?</p>
</blockquote>

<p>This post explains how we extract both the <strong>raw text</strong> and the <strong>structured citations</strong> from reports using two tools:</p>
<ul>
  <li>PyMuPDF for text</li>
  <li>GROBID for citations and metadata</li>
</ul>

<hr />

<h2 id="why-this-matters">Why This Matters</h2>

<p>PDFs are designed for printing, not for reading by machines.</p>

<p>They can include:</p>
<ul>
  <li>Columns and footnotes</li>
  <li>Images, tables, and scanned pages</li>
  <li>Embedded fonts or malformed characters</li>
  <li>Multiple languages in one document</li>
</ul>

<p>If we want to detect entities and link knowledge later, we need high-quality <strong>plain text</strong>.</p>

<hr />

<h2 id="step-1-extract-text-with-pymupdf">Step 1: Extract Text with PyMuPDF</h2>

<p>We use <a href="https://pymupdf.readthedocs.io/"><code class="language-plaintext highlighter-rouge">PyMuPDF</code></a> (also known as <code class="language-plaintext highlighter-rouge">fitz</code>) to extract the text page-by-page from a PDF.</p>

<p>It:</p>
<ul>
  <li>Preserves layout well</li>
  <li>Handles multiple languages</li>
  <li>Works with scanned+OCR‚Äôd documents if text is embedded</li>
</ul>

<p><strong>Example output:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Amazon rainforest is shrinking rapidly.

WWF reported that deforestation increased 12% in 2023.
</code></pre></div></div>

<p>This text gets saved as <code class="language-plaintext highlighter-rouge">report.txt</code>.</p>

<h2 id="step-2-extract-citations-with-grobid">Step 2: Extract Citations with GROBID</h2>

<p>Next, we use <a href="https://github.com/kermitt2/grobid">GROBID</a>, a tool that reads the bibliography and metadata of academic papers and reports.</p>

<p>GROBID converts messy citation lists like:</p>

<blockquote>
  <p>[12] Smith, J., ‚ÄúBiodiversity and Forests‚Äù, Nature, 2020</p>
</blockquote>

<p>Into structured, machine-readable <strong>TEI XML</strong>, which can include:</p>

<ul>
  <li>Title</li>
  <li>Authors</li>
  <li>Year</li>
  <li>Journal or publisher</li>
  <li>DOI or identifiers</li>
</ul>

<p>We save this as <code class="language-plaintext highlighter-rouge">report.biblio.xml</code>.</p>

<p>Later in the pipeline, this will help us:</p>

<ul>
  <li>Build <strong>:CITES relationships</strong> in the graph</li>
  <li>Match references across reports</li>
  <li>Cluster similar reports by their sources</li>
</ul>

<h2 id="how-this-works-in-code">How This Works in Code</h2>

<p>Our system includes a script that does this automatically:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python extract_text.py my-report.pdf
</code></pre></div></div>

<p>It will:</p>

<ol>
  <li>Extract the full text using PyMuPDF ‚Üí <code class="language-plaintext highlighter-rouge">my-report.txt</code></li>
  <li>Send the PDF to the GROBID API ‚Üí <code class="language-plaintext highlighter-rouge">my-report.biblio.xml</code></li>
</ol>

<p>The script runs inside a Docker container and outputs files to the <code class="language-plaintext highlighter-rouge">/data/output/</code> folder.</p>

<h2 id="try-it-yourself">Try It Yourself</h2>

<p>If you‚Äôve followed the setup from <a href="/posts/20_building_blocks/">Part 2</a>, you can run:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make pipeline PDF=my-report.pdf
</code></pre></div></div>

<p>This will:</p>

<ul>
  <li>Extract text and citations</li>
  <li>Tag entities (coming up in Part 4)</li>
  <li>Load data into Neo4j</li>
  <li>Export annotated text for review</li>
</ul>

<p>Check the results in <code class="language-plaintext highlighter-rouge">data/output/</code> ‚Äî you‚Äôll see <code class="language-plaintext highlighter-rouge">.txt</code>, <code class="language-plaintext highlighter-rouge">.entities.json</code>, and <code class="language-plaintext highlighter-rouge">.biblio.xml</code> files.</p>

<hr />

<h2 id="what-we-learned">What We Learned</h2>

<ul>
  <li>PDFs are tricky, but PyMuPDF gives us reliable plain text</li>
  <li>GROBID gives us structured citations, ready for linking</li>
  <li>Clean text is the foundation for everything that follows</li>
</ul>

<hr />

<h2 id="whats-next">What‚Äôs Next?</h2>

<p>In the next post, we‚Äôll explore how we tag entities in the text using AI ‚Äî recognizing organizations, locations, ecological concepts, and more.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[The first step in building a knowledge graph for integral ecology is simple in concept ‚Äî but tricky in practice:]]></summary></entry><entry><title type="html">Building Blocks: Documents, Entities, and Relationships</title><link href="/dlie_knowledge_graph/2025/06/26/How-reports-are-transformed-into-structured,-searchable-networks-of-knowledge" rel="alternate" type="text/html" title="Building Blocks: Documents, Entities, and Relationships" /><published>2025-06-26T00:00:00+00:00</published><updated>2025-06-26T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/06/26/How%20reports%20are%20transformed%20into%20structured,%20searchable%20networks%20of%20knowledge.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/06/26/How-reports-are-transformed-into-structured,-searchable-networks-of-knowledge"><![CDATA[<p>In our last post, we introduced the vision: building a knowledge graph for <strong>integral ecology</strong>, a way to connect people, places, organizations, and ideas across languages and disciplines.</p>

<p>But how exactly do we turn messy PDFs and complex reports into a meaningful, searchable web of knowledge?</p>

<p>We start with three key building blocks:</p>

<hr />

<h2 id="1-documents">1. Documents</h2>

<p>The foundation of our knowledge graph is a <strong>document</strong> ‚Äî a report, academic paper, policy brief, or even a faith-based reflection.</p>

<p>Each document is:</p>

<ul>
  <li>A single file (usually a PDF)</li>
  <li>With a title, language, source (UNEP, WWF, OpenAlex, etc.)</li>
  <li>That contains many sentences ‚Äî and lots of <strong>information hidden in plain text</strong></li>
</ul>

<p>We treat each document as a <strong>node</strong> in the graph, and from there, we extract meaning.</p>

<hr />

<h2 id="2-entities">2. Entities</h2>

<p>Entities are <strong>things that the document talks about</strong> ‚Äî people, organizations, species, places, and ecological concepts.</p>

<p>For example:</p>

<table>
  <thead>
    <tr>
      <th>Entity Text</th>
      <th>Label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Amazon rainforest</td>
      <td><code class="language-plaintext highlighter-rouge">LOCATION</code></td>
    </tr>
    <tr>
      <td>WWF</td>
      <td><code class="language-plaintext highlighter-rouge">ORG</code></td>
    </tr>
    <tr>
      <td>climate resilience</td>
      <td><code class="language-plaintext highlighter-rouge">ECO_CONCEPT</code></td>
    </tr>
    <tr>
      <td>Laudato Si‚Äô</td>
      <td><code class="language-plaintext highlighter-rouge">DOCUMENT</code></td>
    </tr>
  </tbody>
</table>

<p>Our system uses <strong>Natural Language Processing (NLP)</strong> tools to automatically recognize these entities in many languages ‚Äî with models trained on large text collections.</p>

<p>Later, we‚Äôll even <strong>fine-tune our own models</strong> to be more accurate for ecological language.</p>

<hr />

<h2 id="3-relationships">3. Relationships</h2>

<p>The real power of a knowledge graph comes from the <strong>connections between entities</strong> ‚Äî also called <strong>edges</strong> or <strong>relationships</strong>.</p>

<p>Some examples:</p>

<ul>
  <li>A document <strong>MENTIONS</strong> an entity</li>
  <li>A document <strong>CITES</strong> another document</li>
  <li>A concept <strong>IS_RELATED_TO</strong> another concept</li>
  <li>An organization <strong>WORKS_IN</strong> a specific region</li>
</ul>

<p>These relationships turn isolated data points into an <strong>interconnected network</strong> ‚Äî where you can explore patterns, paths, and shared meaning.</p>

<hr />

<h2 id="putting-it-together">Putting It Together</h2>

<p>Here‚Äôs a simple example:</p>

<div class="language-cypher highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ss">[</span><span class="n">WWF</span> <span class="n">Report</span><span class="ss">]</span><span class="err">‚Äì</span><span class="n">MENTIONS</span><span class="err">‚Äì</span><span class="o">&gt;</span><span class="ss">[</span><span class="n">Amazon</span> <span class="n">rainforest</span><span class="ss">]</span>
<span class="err">‚Äì</span><span class="n">MENTIONS</span><span class="err">‚Äì</span><span class="o">&gt;</span><span class="ss">[</span><span class="n">climate</span> <span class="n">resilience</span><span class="ss">]</span>
<span class="err">‚Äì</span><span class="n">CITES</span><span class="err">‚Äî‚Äì</span><span class="o">&gt;</span><span class="ss">[</span><span class="n">IPBES</span> <span class="mi">2022</span> <span class="n">Report</span><span class="ss">]</span>
</code></pre></div></div>

<p>In the graph database, each of these is a <strong>node</strong> (document or entity) and each arrow is a <strong>relationship</strong>.</p>

<p>We can now:</p>

<ul>
  <li>Search for all reports mentioning ‚Äúclimate resilience‚Äù</li>
  <li>Find which NGOs cite a particular scientific assessment</li>
  <li>Map ecological priorities across language and region</li>
</ul>

<hr />

<h2 id="why-it-matters">Why It Matters</h2>

<p>This model gives us:</p>

<ul>
  <li><strong>Structure</strong> ‚Äî so we can search and analyze consistently</li>
  <li><strong>Scalability</strong> ‚Äî works for hundreds or thousands of documents</li>
  <li><strong>Interoperability</strong> ‚Äî can be visualized, queried, and shared</li>
</ul>

<p>And it sets the stage for automation, collaboration, and learning.</p>

<hr />

<h2 id="try-it-yourself-clone--run-the-project">Try It Yourself: Clone &amp; Run the Project</h2>

<p>You can explore and run this pipeline locally using Docker.</p>

<h3 id="prerequisites">Prerequisites</h3>

<ul>
  <li><a href="https://www.docker.com/">Docker</a> (Desktop or CLI)</li>
  <li><a href="https://git-scm.com/">Git</a></li>
</ul>

<h3 id="step-1-clone-the-repository">Step 1: Clone the Repository</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/clirdlf/dlie_knowledge_graph.git
<span class="nb">cd </span>dlie_knowledge_graph
</code></pre></div></div>

<h3 id="step-2-build-and-start-the-system">Step 2: Build and Start the System</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make build
make up
</code></pre></div></div>

<p>This will spin up:</p>

<ul>
  <li><a href="https://grobid.readthedocs.io/en/latest/">GROBID</a> for citation parsing</li>
  <li><a href="https://neo4j.com/">Neo4j</a> for the knowledge graph</li>
  <li><a href="https://doccano.github.io/doccano/">Doccano</a> for annotation</li>
  <li>A Python environment for text and entity extraction</li>
</ul>

<p>Then, you can run the full pipeline like this:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make pipeline <span class="nv">PDF</span><span class="o">=</span>/data/input/202206_IPBES_GLOBALREPORT.pdf
</code></pre></div></div>

<p>You‚Äôll find the results in the <code class="language-plaintext highlighter-rouge">data/output/</code> and <code class="language-plaintext highlighter-rouge">data/doccano/</code> folders.</p>

<h2 id="whats-next">What‚Äôs Next?</h2>

<p>In the next post, we‚Äôll start <strong>extracting text from real reports</strong> ‚Äî even messy PDFs ‚Äî using smart tools like PyMuPDF and GROBID.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In our last post, we introduced the vision: building a knowledge graph for integral ecology, a way to connect people, places, organizations, and ideas across languages and disciplines.]]></summary></entry><entry><title type="html">What is a Knowledge Graph for Integral Ecology?</title><link href="/dlie_knowledge_graph/2025/06/25/Introduction-to-what-we're-building-and-why-it-matters" rel="alternate" type="text/html" title="What is a Knowledge Graph for Integral Ecology?" /><published>2025-06-25T00:00:00+00:00</published><updated>2025-06-25T00:00:00+00:00</updated><id>/dlie_knowledge_graph/2025/06/25/Introduction%20to%20what%20we&apos;re%20building%20and%20why%20it%20matters.</id><content type="html" xml:base="/dlie_knowledge_graph/2025/06/25/Introduction-to-what-we&apos;re-building-and-why-it-matters"><![CDATA[<p>We live in an age of overwhelming information, but ecological knowledge, wisdom, and action often remain <strong>fragmented</strong>, buried in reports, scattered across languages, or locked in formats only specialists can access.</p>

<p>That‚Äôs where the <strong>Digital Library of Integral Ecology</strong> comes in. Our mission is to bring this information together, from scientific papers to NGO reports to faith-based reflections, into a single, interconnected digital library that helps researchers, educators, and communities act for the common good. One of the tools that helps us discovery and interrogate integral ecology is a knowledge graph. This codebase and set of tutorials will walk you through the technologies, opportunities, and tradeoffs in building this feature of the Digitial Library of Integral Ecology.</p>

<hr />

<h2 id="what-do-we-mean-by-integral-ecology">What Do We Mean by ‚ÄúIntegral Ecology‚Äù?</h2>

<p>The term <em>integral ecology</em> comes from <em><a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si‚Äô</a></em>, Pope Francis‚Äô encyclical on the environment. It‚Äôs about recognizing the <strong>deep connections between ecological, social, cultural, and spiritual concerns</strong>. Climate change, deforestation, loss of biodiversity ‚Äî these aren‚Äôt just technical problems. They are moral ones, economic ones, and spiritual ones too.</p>

<p>Integral ecology asks us to <strong>think in systems</strong>, and to see how everything is connected.</p>

<hr />

<h2 id="and-whats-a-knowledge-graph">And What‚Äôs a Knowledge Graph?</h2>

<p>A <strong>knowledge graph</strong> is a way of storing and exploring knowledge by looking at <strong>relationships</strong>. Imagine a big web:</p>

<ul>
  <li>A report mentions <strong>‚ÄúAmazon rainforest‚Äù</strong></li>
  <li>It connects to a <strong>location</strong></li>
  <li>The report is published by <strong>UNESCO</strong></li>
  <li>It discusses concepts like <strong>resilience</strong> and <strong>biodiversity</strong></li>
  <li>It cites other documents that are connected too</li>
</ul>

<p>All of this is represented not just as flat text, but as <strong>linked data</strong> ‚Äî relationships between concepts, people, places, and ideas. This is what lets us ask better questions and see deeper patterns.</p>

<hr />

<h2 id="what-were-building">What We‚Äôre Building</h2>

<p>We are creating a system that:</p>

<ol>
  <li><strong>Extracts text</strong> from ecological reports and scientific papers (even in PDF format)</li>
  <li><strong>Identifies key concepts</strong>, organizations, places, species, and ideas ‚Äî in multiple languages</li>
  <li><strong>Links them together</strong> in a searchable, visual graph (using Neo4j)</li>
  <li><strong>Lets people annotate</strong> and refine that knowledge with simple tools</li>
  <li><strong>Trains smarter AI models</strong> over time that understand ecology more deeply</li>
</ol>

<hr />

<h2 id="why-it-matters">Why It Matters</h2>

<ul>
  <li>Researchers can discover connections across disciplines and languages</li>
  <li>NGOs can map their work to broader systems and goals</li>
  <li>Educators can explore real-world ecological examples interactively</li>
  <li>Communities can build shared understanding of their bioregions</li>
</ul>

<p>We believe this is a project not just of technology ‚Äî but of <strong>ecological conversion</strong>.</p>

<hr />

<h2 id="whats-next">What‚Äôs Next?</h2>

<p>In the next post, we‚Äôll walk through the building blocks of the graph: <strong>documents, entities, and relationships</strong>, and how we transform text into structure.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[We live in an age of overwhelming information, but ecological knowledge, wisdom, and action often remain fragmented, buried in reports, scattered across languages, or locked in formats only specialists can access.]]></summary></entry></feed>